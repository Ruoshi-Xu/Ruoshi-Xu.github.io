<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/icon32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/icon16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"ruoshi.tech","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Escapist&#39;s Blog">
<meta property="og:url" content="http://ruoshi.tech/page/4/index.html">
<meta property="og:site_name" content="Escapist&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Escapist">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://ruoshi.tech/page/4/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Escapist's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Escapist's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-resource">

    <a href="/resources/" rel="section"><i class="fa fa-download fa-fw"></i>resource</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://ruoshi.tech/2024/10/12/Pytorch%20Distributed%20Training(1)--DP_DDP/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/sign.bmp">
      <meta itemprop="name" content="Escapist">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Escapist's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/10/12/Pytorch%20Distributed%20Training(1)--DP_DDP/" class="post-title-link" itemprop="url">Pytorch Distributed Training(1)--DP/DDP</a>
        </h2>

        <div class="post-meta">
		
		  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-10-12 19:40:48" itemprop="dateCreated datePublished" datetime="2024-10-12T19:40:48+08:00">2024-10-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-10-19 14:36:11" itemprop="dateModified" datetime="2024-10-19T14:36:11+08:00">2024-10-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Python-Programming/" itemprop="url" rel="index"><span itemprop="name">Python Programming</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="overview">Overview</h2>
<p>Basically, there are two types of distributed training: data parallelism and model parallelism.</p>
<ul>
<li>Data Parallelism: the model is replicated on each device and each replica processes a different portion of the input data. The gradients are then aggregated across all devices and the model is updated.</li>
<li>Model Parallelism: different parts of the model are placed on different devices and the forward and backward passes are executed in parallel.</li>
</ul>
<p>Note that the parallelism in PyTorch is only supported on Linux.</p>
<h2 id="data-parallelism">Data Parallelism</h2>
<h3 id="data-paralleldp">Data Parallel(DP)</h3>
<p>DP is the simplest way to use multiple GPUs. Only one extra line of code is needed to implement DP.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = torch.nn.DataParallel(model)</span><br></pre></td></tr></table></figure>
<p>The steps are as follows:</p>
<ul>
<li><strong>Forward pass</strong>:
<ul>
<li>Scatter a batch of input data from one assigned GPU to all GPUs. <img src="image-6.png" alt="alt text" /></li>
<li>Replicate the model on all GPUs. <img src="image-7.png" alt="alt text" /></li>
<li>Parallel forward pass on all GPUs. <img src="image-8.png" alt="alt text" /></li>
<li>Gather the output from all GPUs to the assigned GPU. <img src="image-9.png" alt="alt text" /></li>
</ul></li>
<li><strong>Backward pass</strong>:
<ul>
<li>Compute the loss and gradient on the assigned GPU. <img src="image-10.png" alt="alt text" /></li>
<li>Scatter the gradient from the assigned GPU to all GPUs. <img src="image-11.png" alt="alt text" /></li>
<li>Parallel backward pass on all GPUs <img src="image-12.png" alt="alt text" /></li>
<li>Gather the gradient from all GPUs to the assigned GPU and update the model. <img src="image-13.png" alt="alt text" /></li>
</ul></li>
</ul>
<p>Note that the assigned GPU has to accommodate the whole batch of data, and conduct all the work of model updating. Thus the performance of DP is limited concerning both space and time.</p>
<h3 id="distributed-data-parallelddp">Distributed Data Parallel(DDP)</h3>
<h4 id="steps">Steps</h4>
<p>DDP is a more advanced way to use multiple GPUs. It is more flexible and efficient than DP. The steps are as follows: * <strong>Preparation</strong> * Replicate the model on all GPUs and divide the input data among all GPUs equally and randomly. Each GPU load its own data from the disk. * <strong>Training</strong> * Forward: The computation of loss function is done on each GPU without gathering the results on one assigned GPU. * Backward: Each process communicates with each other by All-Reduce operation to exchange the gradients and compute the average gradient. The model is updated on each GPU using the same average gradient. * Updating: Each process has its own optimizer and updates the model on its own GPU. Since the initial value and the gradient are the same, the model on each GPU is also the same.</p>
<p>To further improve the performance, DDP uses a further optimized version of All-Reduce. Obviously, it is not efficient to communicate after finishing all the computation of gradient. Therefore, the model parameters are partitioned into a lot of buckets, Once the gradient computation in bucket <span class="math inline">\(h-1\)</span> is finished, the communication of gradient in bucket <span class="math inline">\(h-1\)</span> and the gradient computation of bucket <span class="math inline">\(h\)</span> begin simultaneously</p>
<h4 id="backend-communication">Backend Communication</h4>
<p>The backend communication of DDP supports diffenrent protocals. The choice of protocals are determined by the factors below</p>
<ul>
<li><strong>Network Environment</strong>
<ul>
<li>Ethernet: <code>nccl</code> has a better performance, <code>gloo</code> is for spare use</li>
<li>InfiniBand: <code>nccl</code> only</li>
</ul></li>
<li><strong>Operators</strong>: <code>nccl</code> supports more diverse operators than <code>gloo</code></li>
</ul>
<p>In practical use, <code>nccl</code> is the prior choice.</p>
<h4 id="initialization">Initialization</h4>
<ul>
<li><strong>TCP mode</strong>: Independently start each process in bash and allocate rank.</li>
<li><strong>ENV mode</strong>: The program can search for required values in required variables</li>
</ul>
<h4 id="start-up">Start Up</h4>
<ul>
<li><code>mp.spawn()</code>: The module <code>mp</code> capsulates the package <code>multiprocessing</code>, not specifically designed for DDP</li>
<li><code>torchrun</code>: Automatically set the configuration of env variables. Only need to set <code>os.environ['CUDA_VISIBLE_DEVICES']</code> manually</li>
<li><code>torch.distributed.launch</code>: To be deprecated</li>
</ul>
<p>Parameters used in the last two ways of startup(i.e., parameters passed through the startup command)</p>
<ul>
<li><code>--nproc_per_node</code>: number of processes per machine</li>
<li><code>--nnodes</code>: number of machines/nodes</li>
<li><code>--node_rank</code>: the idx of this machine</li>
<li><code>--master_addr</code>: the IP of idx-0 machine</li>
<li><code>--master_port</code>: the available port of idx-0 machine</li>
</ul>
<p>Other parameters and concepts that may be used in the program</p>
<ul>
<li><code>node</code>: a machine, which may contain multiple GPUs</li>
<li><code>world_size</code>: number of processes in all nodes, ranging from 1 to <code>world_size</code><span class="math inline">\(-1\)</span></li>
<li><code>local_rank</code>: the rank of the process in the current node. If there are 4 GPUs in one node, the <code>local_rank</code> ranges from 0 to 3</li>
<li><code>rank</code>: the rank of the process in all nodes ### Program Revision(mp.spawn()) #### Import <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import torch.distributed as dist</span><br><span class="line">import torch.multiprocessing as mp</span><br><span class="line">from torch.cuda.amp import GradScaler</span><br><span class="line">from torch.utils.data.distributed import DistributedSampler</span><br><span class="line">from torch.nn.parallel import DistributedDataParallel as DDP</span><br></pre></td></tr></table></figure></li>
<li><code>dist</code>: communication between GPUs</li>
<li><code>mp</code>: DDP startup</li>
<li><code>GradScaler</code>: automatic mixed precision</li>
<li><code>DistributedSampler</code>: Data sampler in a distributed setting</li>
<li><code>DDP</code>: model capsulation and passing</li>
</ul>
<h4 id="key-functions">Key functions</h4>
<ul>
<li>Function <code>init_ddp</code> for initialization <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def init_ddp(local_rank):</span><br><span class="line">    torch.cuda.set_device(local_rank）</span><br><span class="line">    os.environ[&#x27;RANK&#x27;] = str(local_rank)</span><br><span class="line">    dist.init_processgroup(backend=&#x27;nccl&#x27;,init_method=&#x27;env://&#x27;)</span><br></pre></td></tr></table></figure> Initialize the process using <code>nccl</code> as communication protocal and ENV mode for initialization. After initializing with <code>init_ddp</code>, we can get <code>local_rank</code> and <code>world_size</code> in following code easily. <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">local_rank = dist.get_rank() # rank of current process</span><br><span class="line">world_size = dist.get_world_size() # number of processes in current server</span><br></pre></td></tr></table></figure> Given that all processes have the same model, Only one processes is responsible for printing log and saving checkpoint. <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">if local_rank == 0:</span><br><span class="line">    print(f&#x27;Begin validating&#x27;)</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure></li>
<li>Function <code>reduce_tensor</code> to gather data from different processes <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def reduce_tensor(tensor: torch.Tensor):</span><br><span class="line">    rt = tensor.clone()</span><br><span class="line">    dist.all_reduce(rt, op=dist.reduce_op.SUM)</span><br><span class="line">    rt /= dist.get_worldsize)</span><br><span class="line">    return rt</span><br></pre></td></tr></table></figure></li>
<li>Function <code>get_ddp_generator</code> to enhance the randomness <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def get_ddp_generator(seed=3407):</span><br><span class="line">  local_rank = dist.get_rank()</span><br><span class="line">  g = torch.Generator()</span><br><span class="line">  g.manual_seed(seed + local_rank)                       </span><br><span class="line">  return g</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="program-entry">Program Entry</h4>
<ul>
<li>The parameter <code>nprocs</code> should be equal to <code>worldsize</code> otherwise will cause dead lock <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(&#x27;-gpu&#x27;, default=&#x27;0,1,2,3&#x27;, type=str)</span><br><span class="line">    ...</span><br><span class="line">    os.environ[&#x27;MASTER_ADDR&#x27;] = &#x27;localhost&#x27;</span><br><span class="line">    os.environ[&#x27;MASTER_PORT&#x27;] = &#x27;19198&#x27; [&#x27;CUDA_VISIBLE_DEVICES&#x27;] = args.gpu</span><br><span class="line">    world_size = torch.cuda.device_count() </span><br><span class="line">    os.environ[&#x27;WORLD_SIZE&#x27;] = str(world_size)</span><br><span class="line">    os.environ[&#x27;PYTORCH_CUDA_ALLOC_CONF&#x27;] = &quot;max_split_size_mb:128&quot;</span><br><span class="line"></span><br><span class="line">    if args.mode == &#x27;train&#x27;:</span><br><span class="line">        mp.spawn(fn=train, nprocs=world_size, args=(args,))</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="main-function">Main Function</h4>
<ul>
<li>The argument <code>local_rank</code> is automatically allocated by <code>mp.spawn()</code></li>
<li>The function <code>init_ddp()</code> is needed for the initialization of every process <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def main(local_rank, args):</span><br><span class="line">    ...</span><br><span class="line">    init_ddp(local_rank)</span><br><span class="line">    ...</span><br><span class="line">    model.cuda()</span><br><span class="line">    model = nn.SyncBatchnorm.convert_sync_batchnorm(model)</span><br><span class="line">    ...</span><br><span class="line">    num_gpus = torch.cuda.device_count()</span><br><span class="line">    if num_gpus &gt; 1: </span><br><span class="line">        model = nn.parallel.DistributedDataParallel(model, device_id=[local_rank], output_device=local_rank)</span><br><span class="line">    ...</span><br><span class="line">    scaler = GradScaler()</span><br><span class="line">    ...</span><br><span class="line">    for epoch in range(epochs):</span><br><span class="line">        if local_rank == 0:</span><br><span class="line">            ....</span><br><span class="line">        train_dataloader.sampler.set_epoch(epoch)</span><br><span class="line">        train(model, ..., scaler, args)</span><br><span class="line">    dist.destroy_process_group()</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="get_dataloader-function">Get_dataloader Function</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">def get_dataloader(path, args, ..., train: bool):</span><br><span class="line">    ...</span><br><span class="line">    if train:</span><br><span class="line">        train_sampler = DistributedSampler(data, shuffle=True)  </span><br><span class="line">        g = get_ddp_generator()</span><br><span class="line">        dataloader = DataLoader(dataset=data,</span><br><span class="line">                                batch_size=args[&#x27;batch_size&#x27;],</span><br><span class="line">                                num_workers=args[&#x27;num_workers&#x27;],</span><br><span class="line">                                pin_memory=True,</span><br><span class="line">                                shuffle=False, </span><br><span class="line">                                sampler=train_sampler,</span><br><span class="line">                                generator=g)</span><br><span class="line">    else:</span><br><span class="line">        test_sampler = DistributedSampler(data, shuffle=False)  </span><br><span class="line">        dataloader = DataLoader(dataset=data,</span><br><span class="line">                                batch_size=args[&#x27;batch_size&#x27;],</span><br><span class="line">                                num_workers=args[&#x27;num_workers&#x27;],</span><br><span class="line">                                pin_memory=True,</span><br><span class="line">                                shuffle=False,  # 采用顺序采样器</span><br><span class="line">                                sampler=test_sampler)</span><br><span class="line">    </span><br><span class="line">    return dataloader</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="train-function">Train Function</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">def train(...):</span><br><span class="line">    model.train()</span><br><span class="line">    ...</span><br><span class="line">    for step, batch in enumerate(train_dataloader):</span><br><span class="line">        ...</span><br><span class="line">        with torch.cuda.amp.autocast():  </span><br><span class="line">            output = ...</span><br><span class="line">            loss = ...</span><br><span class="line">        ...</span><br><span class="line">        reduced_loss = reduce_tensor(loss.data)  </span><br><span class="line">        if dist.get_rank() == 0:  </span><br><span class="line">            print(...)</span><br><span class="line">        train_loss += reduced_loss.item()</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        scaler.scale(loss).backward()  </span><br><span class="line">        scaler.step(optimizer)  </span><br><span class="line">        scheduler.step()  </span><br><span class="line">        scaler.update()  </span><br><span class="line">        ...</span><br><span class="line">        torch.cuda.empty_cache()  </span><br><span class="line"></span><br><span class="line">    if dist.get_rank() == 0:</span><br><span class="line">        print(...)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="validation-function">Validation Function</h4>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">@torch.no_grad()</span><br><span class="line">def validate(...):</span><br><span class="line">    model.eval()</span><br><span class="line">    ...</span><br><span class="line">    with torch.cuda.amp.autocast():</span><br><span class="line">        output = ...</span><br><span class="line">        </span><br><span class="line">    loss = ...</span><br><span class="line">    reduced_loss = reduce_tensor(loss.data)</span><br><span class="line">    eval_loss += reduced_loss.item()</span><br><span class="line"></span><br><span class="line">    if dist.get_rank() == 0:</span><br><span class="line">        print(...)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    pred_labels = ...</span><br><span class="line">    true_labels = ...</span><br><span class="line">    pred_bools = ...</span><br><span class="line">    macro = ...</span><br><span class="line"></span><br><span class="line">    macro = reduce_tensor(torch.tensor(macro)).cuda()</span><br><span class="line"></span><br><span class="line">return macro</span><br></pre></td></tr></table></figure>
<h3 id="program-revisiontorchrun">Program Revision(torchrun)</h3>
<ul>
<li>Similar to the previous section</li>
<li>No need to import <code>mp</code> and use the capsulation of <code>mp.spawn()</code></li>
<li>Only one env variable needs to be configured in the program</li>
<li><code>local_rank</code> no more serves as an argument in <code>main</code> function. Call <code>os.environ['LOCAL_RANK']</code> to use the variable</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/3/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/5/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Escapist"
      src="/images/sign.bmp">
  <p class="site-author-name" itemprop="name">Escapist</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">5</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/Ruoshi-Xu" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Ruoshi-Xu" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/doc/cv.pdf" title="CV → &#x2F;doc&#x2F;cv.pdf"><i class="fa fa-archive fa-fw"></i>CV</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:xurs2022@mail.sustech.edu.cn" title="E-Mail → mailto:xurs2022@mail.sustech.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Escapist</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>


    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">总访问量<span id="busuanzi_value_site_pv"></span>次</span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">总访客数<span id="busuanzi_value_site_uv"></span>人</span>
    <span class="post-meta-divider">|</span>
<!-- 不蒜子计数初始值纠正 -->
<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});
</script> 



<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/01/2023 10:00:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "本站已运行 "+dnum+" 天 ";
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒.";
    }
setInterval("createtime()",250);
</script>
        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>


  <script defer src="/lib/three/three.min.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
