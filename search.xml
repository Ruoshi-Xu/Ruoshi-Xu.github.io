<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>A Series of Attention Mechanisms</title>
      <link href="/2024/10/08/Paper-Reading/attentions/"/>
      <url>/2024/10/08/Paper-Reading/attentions/</url>
      
        <content type="html"><![CDATA[<h1 id="attention">Attention</h1><h2 id="attention-is-all-you-need">Attention is all you need</h2><h3 id="intro-background">Intro &amp; Background</h3><p>The intro section first talks about RNN-like models and their drawbacks concerning both time and space</p><ul><li>Since RNN-like models have a inherently sequential nature, i.e., <span class="math inline">\(h_t\)</span> is dependent on the hidden state at time <span class="math inline">\(h_{t-1}\)</span>, they are not parallelizable and hence time-consuming.</li><li>RNN-like models also have to store the hidden states of all the previous time steps for back-propagation, which is space-consuming and limits the ability of batch processing.</li></ul><p>Then background section also talks about convolutional networks for sequencial modeling</p><ul><li>CNN operation is totally parallelizable and hence faster than RNN-like models, and it's also space-efficient.</li><li>However, CNN is not good at capturing the dependencies between the output and the input, especially when the distance between the two is large. It has something to do with the concept of receptive field. To capture long-range dependencies, we need to stack multiple layers of CNN, which is not efficient.</li></ul><p>To address the drawbacks mentioned above, the authors propose Transformer, which is based solely on attention mechanisms</p><ul><li>No recurrence is included thus parallelizable</li><li>attention machenism can draw global dependencies between the input and the output no matter how far they are from each other</li></ul><h3 id="scaled-dot-attention">Scaled dot attention</h3><p>The core principle of attention is how to map a query and a set of key-value pairs to an output. The output is a weighted sum of the values. The formula is given by <span class="math display">\[\operatorname{Attention}(Q,K,V)=\operatorname{softmax}({\frac{Q K^{T} }{\sqrt{d_{k} } } })V\]</span> Note that dot-product attention is much faster and more space-efficient in ractice</p><hr /><ul><li>query <span class="math inline">\(\bf q\)</span>: The row of matrix <span class="math inline">\(Q\)</span>, and thus <span class="math inline">\(Q\)</span> has the size of <span class="math inline">\(n\times d_k\)</span>, in which <span class="math inline">\(n\)</span> is the number of queries. The query vector has a dimension of <span class="math inline">\(d_k\)</span> for each corresponding embedded token. <span class="math inline">\(d_k\)</span> is actually much smaller than the dimension of an embedded vector. The vector <span class="math inline">\(\bf q\)</span> is linear projected from the corresponding embedded vector <span class="math inline">\(\bf e\)</span> <span class="math display">\[\bf q=W_q e\]</span> in which <span class="math inline">\(\bf W_q\)</span> is a learnable matrix.</li><li>key <span class="math inline">\(\bf k\)</span>: The key vector also has a dimension of <span class="math inline">\(d_k\)</span> for each unit and can be calculated by <span class="math display">\[\bf k=W_k e\]</span> in which <span class="math inline">\(\bf W_k\)</span> is also learnable</li><li>The dot product of <span class="math inline">\(\bf q\)</span> and <span class="math inline">\(\bf k\)</span>: Intuitively, <span class="math inline">\(\bf q\)</span> serves as an active query on behalf of each unit (token, patch...) and <span class="math inline">\(\bf k\)</span> is a key waiting for query from <span class="math inline">\(\bf q\)</span>. The scalar value <span class="math inline">\(\bf \langle q, k\rangle\)</span> measures the extent to which the unit represented by <span class="math inline">\(\bf q\)</span> matches that represented by <span class="math inline">\(\bf k\)</span>. By backpropagation and optimization, matrices <span class="math inline">\(\bf W_q\)</span> and <span class="math inline">\(\bf W_k\)</span> can capture a proper dependencies between each unit. <span class="math inline">\(\bf \langle q, k\rangle\)</span> is also called attention score</li><li><p>Division of <span class="math inline">\(\sqrt d_k\)</span>: The author says that for large values of <span class="math inline">\(d_k\)</span>, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients</p></li><li>Softmax: The softmax operation is conducted on <span class="math inline">\(\frac{QK^T}{\sqrt{d_k}}\)</span>, which is an <span class="math inline">\(n\times n\)</span> matrix, along the dimension of <span class="math inline">\(\bf k\)</span>, i.e., Given one <span class="math inline">\(\bf q_i\)</span>ï¼Œ <span class="math inline">\(\sum\nolimits_{j = 1}^{ {d_k} } {\left\langle { { {\bf{q}}_i},{ {\bf{k} }_j} } \right\rangle } = 1\)</span></li><li>Mask operation: <span class="math inline">\(\bf q_i\)</span> should only query keys that locate before it, i.e. <span class="math display">\[\left\langle { { {\bf{q} }_i},{ {\bf{k} }_j} } \right\rangle  = \left\{ \begin{array}{l}\left\langle { { {\bf{q} }_i},{ {\bf{k} }_j} } \right\rangle ,i \le j\\0,i &gt; j\end{array} \right.\]</span> The mask operation should be conducted ahead of softmax</li><li><p>value <span class="math inline">\(\bf v\)</span>: The value vector has a dimension of <span class="math inline">\(d_v\)</span>, which equals to the dimension of embedded vectors, for each unit, <span class="math inline">\(\bf v\)</span> is calculated by <span class="math display">\[\bf v=W_ve\]</span> Given that <span class="math inline">\(d_k &lt;&lt; d_v\)</span>, thus for the three matrices <span class="math inline">\({\left( { {W_v}} \right)_{ {d_v} \times {d_v} } },{\left( { {W_k} } \right)_{ {d_k} \times {d_v}}},{\left( { {W_q} } \right)_{ {d_k} \times {d_v} } }\)</span> <span class="math display">\[\operatorname{param}(W_v)&gt;&gt;\operatorname{param}(W_q) + \operatorname{param}(W_k)\]</span> Thus it's better to decompose <span class="math inline">\(W_v\)</span> into the multiplication of two low-rank matrices. Each value <span class="math inline">\(\bf v_i\)</span> is associated with corresponding key <span class="math inline">\(\bf k_i\)</span>. The output of attention for each unit is actually the weighted sum of all value vectors, serving as the variation of embedded vectors <span class="math display">\[{\mathop{\rm Attention}\nolimits} (Q,K,V) = {\mathop{\rm softmax}\nolimits} (\frac{ {Q{K^T} } }{ {\sqrt { {d_k} } } })V = {\left[ { {\mathop{\rm softmax}\nolimits} (\frac{ {Q{K^T} } }{ \sqrt {d_k}  })} \right]_{n \times n} }{V_{n \times {d_v} } } = {\left[ {\begin{array}{c}{\Delta {E_1} }\\{\Delta {E_2} }\\\vdots \\{\Delta {E_n} }\end{array} } \right]_{n \times {d_v} } }\]</span> Calculating <span class="math inline">\(E_i+\Delta E_i\)</span> gives the prediction of word embedding</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper Reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paper Reading </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2024/10/06/hello-world/"/>
      <url>/2024/10/06/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Hi~</p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
