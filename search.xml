<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>PRML Chapter2 Note Cont.</title>
      <link href="/2024/11/02/PRML%20chapter2%20note%20cont/"/>
      <url>/2024/11/02/PRML%20chapter2%20note%20cont/</url>
      
        <content type="html"><![CDATA[<h2 id="exponential-family">Exponential Family</h2><ul><li>Definition: A family of probability distributions is said to be in the exponential family if it can be expressed in the form <span class="math display">\[  p(x | \eta) = h(x) g(\eta) \exp \left\{ \eta^\top u(x) \right\}\]</span> in which<ul><li><span class="math inline">\(\eta\)</span> is the natural parameter of the distribution, determining its position and shape</li><li><span class="math inline">\(u(x)\)</span> is the sufficient statistics, which is a function of the data <span class="math inline">\(x\)</span> that captures all the information in the data relevant to the parameter <span class="math inline">\(\eta\)</span></li><li><span class="math inline">\(h(x)\)</span> is the base measure. It is usually a constant or a simple function of <span class="math inline">\(x\)</span></li><li><span class="math inline">\(g(\eta)\)</span> is the normalization factor ensuring the distribution integrates to 1</li></ul></li><li><p>For Bernoulli distribution, we have <span class="math display">\[  \begin{align*}  p(x | \mu) &amp;= \text{Bern}(x | \mu) = \mu^x (1 - \mu)^{1 - x} \\  &amp;= \exp \{ x \ln \mu + (1 - x) \ln (1 - \mu) \} \\  &amp;= (1 - \mu) \exp \left\{ \ln \left( \frac{\mu}{1 - \mu} \right) x \right\}\end{align*}\]</span> where <span class="math display">\[\begin{align*}   \eta  &amp;= \ln \left( {\frac{\mu }{ {1 - \mu } } } \right) \Leftrightarrow \mu  = \sigma (\eta ) = \frac{1}{ {1 + \exp ( - \eta )} }\\  u(x) &amp;= x \\h(x)&amp;  = 1 \\ g(\eta) &amp;= 1-\mu =\frac{1}{1+\exp(\eta)} =\sigma(-\eta)\end{align*}\]</span> Thus, Bernoulli distribution can also be expressed in the form of exponential family <span class="math display">\[p(x|\eta)=\sigma(-\eta)\exp(\eta x)\]</span></p></li><li><p>Multinomial distribution <span class="math display">\[  \begin{align*}  p(\mathbf{x} | \mathbf{\mu}) &amp;= \text{Mult}(\mathbf{x} | \mathbf{\mu}) = \prod_{k = 1}^M \mu_k^{x_k} \\  &amp;= \exp \left\{ \sum_{k = 1}^M x_k \ln \mu_k \right\} \\\end{align*}\]</span> The parameters are respectively <span class="math display">\[\eta_k =\ln \mu_k,u(\mathbf{x}) = \mathbf{x},h(\mathbf{x}) = 1,g(\mathbf{\eta}) = 1\]</span> Note that <span class="math inline">\(\eta_k\)</span> is not independent, since <span class="math inline">\(\sum_{k=1}^M \mu_k = 1\)</span>, so we have only <span class="math inline">\(M-1\)</span> independent parameters and <span class="math inline">\(\mu_M\)</span> can be expressed by previous <span class="math inline">\(M-1\)</span> parameters. Redifine <span class="math display">\[\begin{align*}{\mu _M} &amp;= 1 - \sum\limits_{k = 1}^{M - 1} { {\mu _k} } \\{\eta _k} &amp;= \ln \frac{ { {\mu _k} } }{1 - \sum_{j = 1}^{M - 1} { {\mu _j} } } \Leftrightarrow {\mu _k} = \frac{ {\exp ({\eta _k})} }{ {1 + \sum_{j = 1}^{M - 1} {\exp } ({\eta _j})} }\end{align*}\]</span> Thus, the multinomial distribution can also be expressed in the form of exponential family <span class="math display">\[p(\mathbf{x}|\mathbf{\eta})=\exp(\mathbf{\eta}^\top \mathbf{x})\frac{1}{1+\sum_{j=1}^{M-1}\exp(\eta_j)}\]</span> The paremeters are respectively <span class="math display">\[\begin{align*}\eta_k &amp;= \ln \frac{\mu_k}{1-\sum_{j=1}^{M-1}\mu_j}, \eta  = {({\eta _1}, \ldots ,{\eta _{M - 1} },0)^{\rm{T} } }\\u(\mathbf{x}) &amp;= \mathbf{x}\\h(\mathbf{x}) &amp;= 1\\g(\mathbf{\eta}) &amp;= \frac{1}{1+\sum_{j=1}^{M-1}\exp(\eta_j)}\end{align*}\]</span></p></li><li><p>Gaussian distribution <span class="math display">\[\begin{align*}  p(x | \mu, \sigma^2) &amp;= \frac{1}{(2 \pi \sigma^2)^{1/2} } \exp \left\{ -\frac{1}{2 \sigma^2} (x - \mu)^2 \right\} \\  &amp;= \frac{1}{(2 \pi \sigma^2)^{1/2} } \exp \left\{ -\frac{1}{2 \sigma^2} x^2 + \frac{\mu}{\sigma^2} x - \frac{1}{2 \sigma^2} \mu^2 \right\} \\  &amp;= h(x) g(\eta) \exp \left\{ \eta^T u(x) \right\}\end{align*}\]</span> where the parameters are respectively <span class="math display">\[\begin{align*}  \eta = \begin{bmatrix} \frac{\mu}{\sigma^2} \\ -\frac{1}{2 \sigma^2} \end{bmatrix},   u(x) = \begin{bmatrix} x \\ x^2 \end{bmatrix},   h(x) = (2 \pi)^{-1/2},   g(\eta) = (-2 \eta_2)^{-1/2} \exp \left( \frac{\eta_1^2}{4 \eta_2} \right)\end{align*}\]</span></p></li></ul><h2 id="maximum-likelihood-for-exponential-family">Maximum Likelihood for Exponential Family</h2><p>First, we will establish an important relationship which is useful in future derivation. <span class="math display">\[\begin{align*}    p(\mathbf{x}|\boldsymbol{\eta}) &amp;= h(\mathbf{x})g(\boldsymbol{\eta})\exp \left\{ \boldsymbol{\eta}^\top \mathbf{u}(\mathbf{x}) \right\} \\    &amp;\Rightarrow \int h(\mathbf{x})g(\boldsymbol{\eta})\exp \left\{ \boldsymbol{\eta}^\top \mathbf{u}(\mathbf{x}) \right\} d\mathbf{x} = \int p(\mathbf{x}|\boldsymbol{\eta}) d\mathbf{x} = 1 \\    &amp;\Rightarrow \frac{\partial}{\partial g(\boldsymbol{\eta})} \left( \int h(\mathbf{x})g(\boldsymbol{\eta})\exp \left\{ \boldsymbol{\eta}^\top \mathbf{u}(\mathbf{x}) \right\} d\mathbf{x} \right) \\&amp;= \nabla g(\boldsymbol{\eta}) \int h(\mathbf{x}) \exp \left\{ \boldsymbol{\eta}^\top \mathbf{u}(\mathbf{x}) \right\} d\mathbf{x}  + g(\boldsymbol{\eta}) \int h(\mathbf{x}) \exp \left\{ \boldsymbol{\eta}^\top \mathbf{u}(\mathbf{x}) \right\} \mathbf{u}(\mathbf{x}) d\mathbf{x} \\    &amp;= 0\end{align*}\]</span> Notice that <span class="math display">\[\begin{align*}    p(\mathbf{x}|\boldsymbol{\eta}) &amp;= h(\mathbf{x})g(\boldsymbol{\eta})\exp \left\{ \boldsymbol{\eta}^\top \mathbf{u}(\mathbf{x}) \right\} \\    &amp;\Rightarrow \frac{p(\mathbf{x}|\boldsymbol{\eta})}{g(\boldsymbol{\eta})} = h(\mathbf{x})\exp \left\{ \boldsymbol{\eta}^\top \mathbf{u}(\mathbf{x}) \right\} \\    &amp;\Rightarrow \int \frac{p(\mathbf{x}|\boldsymbol{\eta})}{g(\boldsymbol{\eta})} d\mathbf{x} = \int h(\mathbf{x})\exp \left\{ \boldsymbol{\eta}^\top \mathbf{u}(\mathbf{x}) \right\} d\mathbf{x} = \frac{1}{g(\boldsymbol{\eta})}\end{align*}\]</span> and <span class="math display">\[\begin{align*}    g(\boldsymbol{\eta}) \int h(\mathbf{x}) \exp \left\{ \boldsymbol{\eta}^\top \mathbf{u}(\mathbf{x}) \right\} \mathbf{u}(\mathbf{x}) d\mathbf{x} &amp;= \int h(\mathbf{x}) g(\boldsymbol{\eta}) \exp \left\{ \boldsymbol{\eta}^\top \mathbf{u}(\mathbf{x}) \right\} \mathbf{u}(\mathbf{x}) d\mathbf{x} \\    &amp;= \int p(\mathbf{x}|\boldsymbol{\eta}) \mathbf{u}(\mathbf{x}) d\mathbf{x} \\    &amp;= \mathbb{E}\left[ \mathbf{u}(\mathbf{x}) \right]\end{align*}\]</span> Substitute the above two equations into the previous equation, we have <span class="math display">\[-\nabla\ln g(\boldsymbol{\eta})={E}[\mathbf{u}(\mathbf{x})]\]</span></p><p>Then, re-consider the likelihood function of the exponential family to get the same result. Given the dataset <span class="math inline">\(\mathbf{X} = \{ \mathbf{x}_1, \dots, \mathbf{x}_N \}\)</span>, the likelihood function is <span class="math display">\[ p(\mathbf{X}|\boldsymbol{\eta}) = \prod_{n=1}^N p(\mathbf{x}_n|\boldsymbol{\eta}) = \left( \prod_{n=1}^N h(\mathbf{x}_n) \right) g(\boldsymbol{\eta})^N \exp \left\{ \boldsymbol{\eta}^\top \sum_{n=1}^N \mathbf{u}(\mathbf{x}_n) \right\}\]</span> Take the logarithm of the likelihood function and maximize it with respect to <span class="math inline">\(\boldsymbol{\eta}\)</span>, we have <span class="math display">\[\begin{align*}    \ell(\boldsymbol{\eta}) &amp;= \log p(\mathbf{X}|\boldsymbol{\eta}) \\    &amp;= \sum_{n=1}^N \log h(\mathbf{x}_n) + N \ln g(\boldsymbol{\eta}) + \boldsymbol{\eta}^\top \sum_{n=1}^N \mathbf{u}(\mathbf{x}_n).\\&amp;\Rightarrow    \frac{\partial \ell(\boldsymbol{\eta})}{\partial \boldsymbol{\eta} } = N \frac{\nabla g(\boldsymbol{\eta})}{g(\boldsymbol{\eta})} + \sum_{n=1}^N \mathbf{u}(\mathbf{x}_n) = 0.\\ &amp;\Rightarrow -\nabla \ln g(\boldsymbol{\eta}) = \mathbb{E}[\mathbf{u}(\mathbf{x})]\\&amp;\Rightarrow    -\nabla \ln g(\boldsymbol{\eta}_{\text{ML} }) = \frac{1}{N} \sum_{n=1}^N \mathbf{u}(\mathbf{x}_n).\end{align*}\]</span></p><h2 id="conjugate-prior-for-exponential-family">Conjugate Prior for Exponential Family</h2><p>For any member of the exponential family, there exists a conjugate prior that ensures the posterior distribution has the same form as the prior. This prior distribution can be written as: <span class="math display">\[p(\boldsymbol{\eta} | \boldsymbol{\chi}, \nu) = f(\boldsymbol{\chi}, \nu) g(\boldsymbol{\eta})^\nu \exp \left\{ \nu \boldsymbol{\eta}^\top \boldsymbol{\chi} \right\}\]</span> in which * <span class="math inline">\(\boldsymbol{\chi}\)</span> represents the value of a virtual or pseudo-observation in the prior * <span class="math inline">\(\nu\)</span> is the number of observations, which is a hyperparameter. It is often called the &quot;strength&quot; or &quot;pseudo-count&quot; of the prior, indicating the weight of prior information.</p><p>In Bayesian inference, we combine the likelihood function with the prior distribution to obtain the posterior distribution. Given a dataset <span class="math inline">\(\mathbf{X}=\{\mathbf{x}_{1},\cdots,\mathbf{x}_{N}\}\)</span>, the likelihood function is <span class="math display">\[p(\mathbf{X}|\boldsymbol{\eta}) \propto g(\boldsymbol{\eta})^N \exp \left\{ \boldsymbol{\eta}^\top \sum_{n=1}^N \mathbf{u}(\mathbf{x}_n) \right\}\]</span> Combining this likelihood with the prior distribution yields the posterior distribution <span class="math display">\[p(\boldsymbol{\eta}|\mathbf{X}, \boldsymbol{\chi}, \nu) \propto g(\boldsymbol{\eta})^{\nu + N} \exp \left\{ \boldsymbol{\eta}^\top \left( \sum_{n=1}^N \mathbf{u}(\mathbf{x}_n) + \nu \boldsymbol{\chi} \right) \right\}\]</span></p>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PRML Chapter2 Note</title>
      <link href="/2024/10/17/PRML%20chapter2%20note/"/>
      <url>/2024/10/17/PRML%20chapter2%20note/</url>
      
        <content type="html"><![CDATA[<h2 id="beta-distribution">Beta Distribution</h2><ul><li>Beta distribution is a continuous probability distribution defined on the interval $[0, 1] $</li><li>Two positive parameters, denoted by <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span></li><li>The definition of this distribution is <span class="math display">\[\begin{aligned}&amp; {\rm{Beta} }(x |a,b)   =   \frac{ {\Gamma (a + b)} }{ {\Gamma (a)\Gamma (b)} }{x ^{a - 1} }{(1 - x )^{b - 1} }\\&amp; {\rm {E} }[x ]   =   \frac{a}{ {a + b} }\\&amp; {\rm{var} }[x ] = \frac{ {ab} }{ { { {(a + b)}^2}(a + b + 1)} }\end{aligned} \]</span> in which <span class="math display">\[\Gamma (x) =\int_{0}^{\infty}t^{x-1}e^{-t}\mathrm{d}t=(x-1)!\]</span> The shapes of distributions are totally different for different values <img src="image.png" alt="alt text" /></li><li>The Beta distribution provides the conjugate prior for the Bernoulli distribution<ul><li><strong>conjugate prior</strong>: If a prior distribution is the conjugate prior of a distribution, the result of Bayesian updating (i.e., combining the likelihood with the prior) will still be a distribution of the same type, but with updated parameters.</li><li>If we use a Beta distribution as the prior for the Bernoulli distribution, after observing data, the posterior distribution will still be a Beta distribution, but with its parameters updated based on the data <span class="math display">\[\begin{aligned}p({\mu ^*}|{a_0},{b_0},D)&amp; = \frac{ {p(D,{\mu ^*}|{a_0},{b_0})} }{ {\int {p(D|\mu )p(\mu |{a_0},{b_0}){\mkern 1mu} d\mu } } }\\ &amp;= \frac{ {p(D|{\mu ^*})p({\mu ^*}|{a_0},{b_0})} }{ {P(D)} }\\ &amp;\propto p(D|{\mu ^*})p({\mu ^*}|{a_0},{b_0})\\ &amp;= \left( {\prod\limits_{n = 1}^N { {\mu ^{* ~{x_n} } } } { {(1 - \mu^* )}^{ {1 - {x_n} } } } } \right){\rm{Beta} }({\mu ^*}|{a_0},{b_0})\\ &amp;  = {\mu ^*}^m{(1 - {\mu ^*})^{N - m} }{\mu ^*}^{ {a_0} - 1}{(1 - {\mu ^*})^{ {b_0} - 1} }\\ &amp;\propto {\mu ^{*~m + {a_0} - 1} }{(1 - \mu^* )^{N - m + {b_0} - 1} } \\ &amp;\propto {\rm{Beta} }\left( {\mu^* |{a_0} + m,N - m + {b_0} } \right)\end{aligned} \]</span> <span class="math inline">\(\mu^*\)</span> is the variable for query, i.e., we want to query the probability of <span class="math inline">\(E[x]=\mu^*\)</span> given the data <span class="math inline">\(D\)</span> and the prior <span class="math inline">\(p(\mu)={\rm{Beta} }(\mu|a_0,b_0)\)</span>. Bayesian method is applied here. <span class="math inline">\(x_n\)</span> is the sampled value in <span class="math inline">\(D\)</span>, being either <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> <img src="image-1.png" alt="alt text" /></li><li>Assign <span class="math inline">\(a_N=a_0 + m\)</span>, <span class="math inline">\(b_N=N-m+b_0\)</span>, as the size of data <span class="math inline">\(N\rightarrow \infty\)</span>, both <span class="math inline">\(m\rightarrow \infty\)</span> and <span class="math inline">\(N-m\rightarrow \infty\)</span>, thus the constant <span class="math inline">\(a_0\)</span> and <span class="math inline">\(b_0\)</span> can be ignored. <span class="math display">\[\begin{aligned}&amp;{a_N} \to m\\&amp;{b_N} \to N - m\\&amp;E(\mu )  =   \frac{ { {a_N} } }{ { {a_N} + {b_N} } }{\mkern 1mu}  \to {\mkern 1mu} \frac{m}{N} = {\mu _{ {\rm{ML} } } }\\&amp;{\mathop{\rm var} } [\mu ] = \frac{ { {a_N}{b_N} } }{ { { {({a_N} + {b_N})}^2}({a_N} + {b_N} + 1)} } \to 0\end{aligned} \]</span></li></ul></li><li>Prediction under the posterior: The probability of the r.v. <span class="math inline">\(x=1\)</span> equals to the integration of <span class="math inline">\(p(x=1)\)</span> under all possible <span class="math inline">\(\mu\)</span> <span class="math display">\[\begin{aligned}&amp;p(x = 1|{a_0},{b_0},D)  \\ &amp;=  \int_0^1 p (x = 1|\mu )p(\mu |{a_0},{b_0},D){\mkern 1mu} {\rm{d} }\mu \\ &amp;= \int_0^1 \mu  p(\mu |{a_0},{b_0},D){\mkern 1mu} {\rm{d} }\mu \\ &amp;= E[\mu |{a_0},{b_0},D]\\ &amp;= \frac{ { {a_N} } }{ { {a_N} + {b_N} } }\\\end{aligned} \]</span></li></ul><h2 id="multinoimal-distribution">Multinoimal Distribution</h2><ul><li><p>Take <span class="math inline">\(1\)</span> out of <span class="math inline">\(K\)</span> one-hot coding as an example, here <span class="math inline">\(K=6\)</span> <span class="math display">\[{\bf x}\ =\,(0,0,1,0,0,0)^{\mathrm{T} }\]</span> The probability of getting such an <span class="math inline">\(\bf x\)</span> is <span class="math display">\[p(\mathbf{x}|\mu)=\prod_{k=1}^{K}\mu_{k}^{x_{k} }\]</span> in which <span class="math inline">\(\mu_k\)</span> refers to the expectation probability of getting a <span class="math inline">\(1\)</span> in the <span class="math inline">\(k\)</span>'th position and <span class="math inline">\(x_k\)</span> is the real value in <span class="math inline">\(\bf x\)</span>'s <span class="math inline">\(k\)</span>'th position. In one-hot coding, only one element is <span class="math inline">\(1\)</span> and the rest are <span class="math inline">\(0\)</span>. Suppose that the <span class="math inline">\(k\)</span>'th element is <span class="math inline">\(1\)</span>, the formula above reduces to <span class="math display">\[ p ({\bf{x} }|\mu ) = { {\mu _k} }  \]</span> Then calculate the expectation <span class="math display">\[E[{\bf{x} }|\mu ] = \sum\limits_{\bf{x} } p ({\bf{x} }|\mu ){\bf{x} } = {({\mu _1}, \ldots ,{\mu _K})^{\rm{T} } } = \bf {\mu}  \]</span></p></li><li>Given an observation set <span class="math inline">\(D=\lbrace\mathbf{x}_{1},\cdot\cdot\cdot,\mathbf{x}_{N}\rbrace\)</span>, the likelihood function and log-likelihood is <span class="math display">\[\begin{aligned}p(D|\mu ) &amp;= \prod\limits_{n = 1}^N {\prod\limits_{k = 1}^K {\mu _k^{ {x_{nk} } } } }  = \prod\limits_{k = 1}^K {\mu _k^{(\sum\limits_n { {x_{nk} } } )} }  = \prod\limits_{k = 1}^K {\mu _k^{ {m_k} } } \\ &amp;\Rightarrow \log p(D|\mu ) = \sum\limits_{k = 1}^K { {m_k} } \log {\mu _k}\end{aligned} \]</span> where <span class="math inline">\(m_k\)</span> is the number of vectors in <span class="math inline">\(D\)</span> satisfying the <span class="math inline">\(k\)</span>'th bit is <span class="math inline">\(1\)</span>. To maximize the likelihood function, we use the Lagrange multiplier method. Since we have the constraint <span class="math inline">\(\sum_{k=1}^{K}\mu_k=1\)</span>, we can construct the Lagrange function <span class="math display">\[\mathcal{L}(\mu,\lambda)=\sum_{k=1}^{K}m_{k}\ln\mu_{k}+\lambda\left(\sum_{k=1}^{K}\mu_{k}-1\right)\]</span> Take the partial derivative of <span class="math inline">\(\mathcal{L}\)</span> with respect to <span class="math inline">\(\mu_{k}\)</span> and set it to <span class="math inline">\(0\)</span> <span class="math display">\[\frac{ {\partial {\cal L} } }{ {\partial {\mu _k} } } = \frac{ { {m_k} } }{ { {\mu _k} } } + \lambda  = 0 \Rightarrow {\mu _k} =  - \frac{ { {m_k} } }{\lambda } \]</span> Then substitute <span class="math inline">\(\mu_{k}\)</span> back to the constraint, we have <span class="math display">\[\sum\limits_{k = 1}^K { {\mu _k} }  = \sum\limits_{k = 1}^K  -  \frac{ { {m_k} } }{\lambda } = 1 \Rightarrow \lambda  =  - \sum\limits_{k = 1}^K { {m_k} }  =  - N \]</span> Thus the maximum likelihood estimation is <span class="math display">\[\mu _k^{ {\rm{ML} } } = \frac{ {m_k} }{N} \]</span></li><li><p>The definition of multinomial distribution and concerning statistical parameters are below <span class="math display">\[\begin{aligned}\text{Mult}(m_1, m_2, \dots, m_K \mid \boldsymbol{\mu}, N) &amp;= \binom{N}{m_1, m_2, \dots, m_K} \prod_{k=1}^{K} \mu_k^{m_k}\\{E}[m_k] &amp;= N \mu_k\\\text{var}[m_k] &amp;= N \mu_k (1 - \mu_k)\\\text{cov}[m_j, m_k] &amp;= -N \mu_j \mu_k\end{aligned}\]</span></p></li></ul><h2 id="dirichlet-distribution">Dirichlet Distribution</h2><ul><li><p>The Dirichlet distribution is a multivariate generalization of the Beta distribution, also the conjugate prior for the multinomial distribution. The definition is <span class="math display">\[\mathrm{Dir}(\mu|\alpha)={\frac{\Gamma(\alpha_{0})}{\Gamma(\alpha_{1})\cdot\cdot\cdot\Gamma(\alpha_{K})} }\prod_{k=1}^{K}\mu_{k}^{\alpha_{k}-1}\]</span> where the parameters satisfy <span class="math display">\[\alpha_{0}=\sum_{k=1}^{K}\alpha_{k}\]</span></p></li><li><p>The derivation showing the Dirichlet distribution is the conjugate prior for the multinomial distribution is below. Given the data set <span class="math inline">\(D=\{m_{1},m_{2},\cdot\cdot\cdot,m_{K}\}\)</span>, where <span class="math inline">\(m_{k}\)</span> is the count of observations in the <span class="math inline">\(k\)</span>'th category. Parameter <span class="math inline">\(\bf{\mu}=\{\mu_1, \mu_2, ..., \mu_K\}\)</span>. The likelihood function is <span class="math display">\[p(D\mid\mu)=\frac{ {N}!}{ {m}_{1}! m_{2}! \cdot \cdot \cdot {m}_{K}!}\prod_{k=1}^{K}\mu_{k}^{m_{k} }\]</span> The Dirichlet prior with parameter <span class="math inline">\(\alpha=\{\alpha_{1},\alpha_{2},\cdot\cdot\cdot,\alpha_{K}\}\)</span> is <span class="math display">\[p(\boldsymbol{\mu} \mid \boldsymbol{\alpha})=\frac{\Gamma\left(\alpha_{0}\right)}{\prod_{k=1}^{K} \Gamma\left(\alpha_{k}\right)} \prod_{k=1}^{K} \mu_{k}^{\alpha_{k}-1}\]</span> Similar to the derivation in last section, apply Bayes' theorem, we have <span class="math display">\[\begin{aligned}p(\boldsymbol{\mu} \mid {D}, \boldsymbol{\alpha}) &amp;\propto p({D} \mid \boldsymbol{\mu}) \, p(\boldsymbol{\mu} \mid \boldsymbol{\alpha}) \\&amp;= \left( \prod_{k=1}^{K} \mu_k^{m_k} \right) \left( \prod_{k=1}^{K} \mu_k^{\alpha_k - 1} \right) \\&amp;= \prod_{k=1}^{K} \mu_k^{m_k + \alpha_k - 1} \\&amp;= \frac{\Gamma(\sum_{k=1}^{K} (\alpha_k + m_k))}{\prod_{k=1}^{K} \Gamma(\alpha_k + m_k)} \prod_{k=1}^{K} \mu_k^{m_k + \alpha_k - 1} \\&amp;=\frac{ {\Gamma ({\alpha _0} + N)} }{ {\prod\limits_{k = 1}^K \Gamma  ({\alpha _k} + {m_k})} }\prod\limits_{k = 1}^K {\mu _k^{ {m_k} + {\alpha _k} - 1} } \\&amp;= \text{Dir}(\boldsymbol{\mu} \mid \boldsymbol{\alpha} + \boldsymbol{m})\end{aligned}\]</span></p></li></ul><h2 id="gaussian-distribution">Gaussian Distribution</h2><ul><li><p>The Gaussian distribution is defined as <span class="math display">\[\mathcal{N}(x|\mu,\sigma^{2})=\frac{1}{(2\pi\sigma^{2})^{1/2} }\exp\left\{-\frac{1}{2\sigma^{2} }(x-\mu)^{2}\right\}\]</span> while multivariate Gaussian distribution is a little bit different <span class="math display">\[{\mathcal N}({\bf x}|\mu,{\bf\Sigma})=\frac{1}{(2\pi)^{D/2} }\frac{1}{|{\bf\Sigma}|^{1/2} }\exp\left\{-\frac{1}{2}({\bf x}-\mu)^{\mathrm{T} }{\bf\Sigma}^{-1}({\bf x}-\mu)\right\}\]</span> The level graph diagram of multivariate Gaussian is shown below <img src="image-2.png" alt="alt text" /></p></li><li><p>Geometry of Multivariate Gaussian<img src="image-3.png" alt="alt text" /> The Mahalanobis Distance <span class="math inline">\(Œî^2\)</span> is defined as <span class="math display">\[\Delta^{2}=(\mathbf{x}-{\boldsymbol{\mu} })^{\mathrm{T} }\Sigma^{-1}(\mathbf{x}-{\boldsymbol{\mu} })\]</span> This equation represents the Mahalanobis distance between a point <span class="math inline">\(x\)</span> and the mean <span class="math inline">\(Œº\)</span> of the distribution. It measures the distance accounting for the distribution's covariance structure. The inverse of the covariance matrix can be acquired through the eigendecomposition of the covariance matrix <span class="math display">\[\Sigma^{-1}=\sum_{i=1}^{D}\frac{1}{\lambda_{i} }\mathbf{u}_{i}\mathbf{u}_{i}^{\mathrm{ {T} } }\]</span> where <span class="math inline">\(\bm u_i\)</span> is the eigenvalue of both <span class="math inline">\(\Sigma\)</span> and <span class="math inline">\(\Sigma^{-1}\)</span>. The Mahalanobis distance can be rewritten as <span class="math display">\[\begin{aligned}{\Delta ^2} &amp;= {(x - \mu )^T}{\mkern 1mu} {\Sigma ^{ - 1} }(x - \mu )\\ &amp;= {(x - \mu )^T}{\mkern 1mu} U{\Lambda ^{ - 1} }{U^T}(x - \mu )\\ &amp;= {y^T}{\Lambda ^{ - 1} }y\\ &amp;= \sum\limits_{i = 1}^D {\frac{ {y_i^2} }{ { {\lambda _i} } } } \end{aligned} \]</span> in which <span class="math display">\[{y_i} = \bm{u_i^T(x - \mu )} \]</span></p></li><li><p>Calculate the expectation: Change the variable, then split the original integration into an odd one and a normal Gaussian <span class="math display">\[\begin{aligned}\mathbb{E}[\mathbf{x}] &amp;= \frac{1}{(2\pi)^{D/2} |\Sigma|^{1/2} } \int \mathbf{x} \exp \left( -\frac{1}{2} (\mathbf{x} - \mu)^\top \Sigma^{-1} (\mathbf{x} - \mu) \right) d\mathbf{x} \\&amp;= \frac{1}{(2\pi)^{D/2} |\Sigma|^{1/2} } \int (\mathbf{z} + \mu) \exp \left( -\frac{1}{2} \mathbf{z}^\top \Sigma^{-1} \mathbf{z} \right) d\mathbf{z} \\&amp;= \frac{1}{(2\pi)^{D/2} |\Sigma|^{1/2} } \int \mathbf{z} \exp \left( -\frac{1}{2} \mathbf{z}^\top \Sigma^{-1} \mathbf{z} \right) d\mathbf{z} + \mu \frac{1}{(2\pi)^{D/2} |\Sigma|^{1/2} } \int \exp \left( -\frac{1}{2} \mathbf{z}^\top \Sigma^{-1} \mathbf{z} \right) d\mathbf{z} \\&amp;= 0 + \mu \cdot 1 \\&amp;= \mu\end{aligned}\]</span></p></li><li>There some other properties<ul><li><p>$[ ^T] =  ^T +  $ <strong>proof</strong>: <span class="math display">\[\begin{aligned}E\left[ { {\bf{x} }{ {\bf{x} }^T} } \right] &amp;= E{\mkern 1mu} \left[ {({\bf{x} } - \mu  + \mu ){ {({\bf{x} } - \mu  + \mu )}^T} } \right]\\ &amp;= E\left[ {({\bf{x} } - \mu ){ {({\bf{x} } - \mu )}^T} } \right] + E\left[ {({\bf{x} } - \mu ){\mu ^T} } \right] + E\left[ {\mu { {({\bf{x} } - \mu )}^T} } \right] + E\left[ {\mu {\mu ^T} } \right]\\ &amp;= \Sigma  + 0 + 0 + \mu {\mu ^T}\\ &amp;= \Sigma  + \mu {\mu ^T}\end{aligned} \]</span></p></li><li><p><span class="math inline">\(\operatorname{cov}[\mathbf{A}{}\mathbf{x}]=\mathbf{A}{}\mathbf{A}^{T}\)</span> <strong>proof</strong>: <span class="math display">\[\begin{aligned}{\mathop{\rm cov} } [{\bf{Ax} }] &amp;= E\left[ {({\bf{Ax} } - E[{\bf{Ax} }]){ {({\bf{Ax} } - E[{\bf{Ax} }])}^T} } \right]\\ &amp;= E\left[ { {\bf{A} }({\bf{x} } - E[{\bf{x} }]){ {({\bf{x} } - E[{\bf{x} }])}^T}{ {\bf{A} }^T} } \right]\\ &amp;= {\bf{A} }E\left[ {({\bf{x} } - E[{\bf{x} }]){ {({\bf{x} } - E[{\bf{x} }])}^T} } \right]{ {\bf{A} }^T}\\ &amp;= {\bf{A} }\Sigma { {\bf{A} }^T}\end{aligned} \]</span></p></li></ul></li><li>Properties of Gaussian<ul><li>Linear <span class="math display">\[\begin{aligned}&amp;X \sim \mathcal{N}(\mu, \sigma^2), \quad Y = aX + b \Rightarrow Y \sim \mathcal{N}(a\mu + b, a^2 \sigma^2)\\&amp;X \sim \mathcal{N}(\mu, \Sigma), \quad Y = AX + B \Rightarrow Y \sim \mathcal{N}(A\mu + B, A \Sigma A^T)\end{aligned}\]</span></li><li>Multiplication <span class="math display">\[\begin{aligned}&amp;X_1 \sim \mathcal{N}(\mu_1, \sigma_1^2), \quad X_2 \sim \mathcal{N}(\mu_2, \sigma_2^2) \Rightarrow p(X_1) \cdot p(X_2) \sim \mathcal{N} \left( {\frac{ {\sigma_{2} }^{2} }{ { {\sigma_{1} }^{2} }+{ {\sigma_{2} }^{2} } } }\,{\mu_{1} }+{\frac{ {\sigma_{1} }^{2} }{ {\sigma_{1} }^{2}+{ {\sigma_{2} }^{2} } } }\,{\mu_{2} }, \frac{1}{\sigma_1^{-2} + \sigma_2^{-2} } \right)\\&amp;X_1 \sim \mathcal{N}(\mu_1, \Sigma_1), \quad X_2 \sim \mathcal{N}(\mu_2, \Sigma_2) \Rightarrow p(X_1) \cdot p(X_2) \sim \mathcal{N} \left({\frac{\Sigma_{2} }{\Sigma_{1}+\Sigma_{2} } }\,\mu_{1}+{\frac{\Sigma_{1} }{\Sigma_{1}+\Sigma_{2} } }\,\mu_{2} , \frac{1}{\Sigma_1^{-1} + \Sigma_2^{-1} }  \right)\end{aligned} \]</span></li></ul></li></ul><h2 id="bayesian-for-gaussion">Bayesian for Gaussion</h2><p>Let's formulate the problem first. Given that <span class="math display">\[\begin{aligned}&amp;y = Ax + v\\&amp;p(x) = {\cal N}(x|\mu ,\Sigma )\quad  p(v) = {\cal N}(v|0,Q)\end{aligned} \]</span> we know <span class="math display">\[\begin{aligned}&amp;p(y|x) = {\cal N}(y|Ax,Q)\\&amp;p(y) = {\cal N}(y|A\mu ,A\Sigma {A^T} + Q)\end{aligned} \]</span> and our task is to estimate <span class="math inline">\(p(x|y)\)</span>. Assume that <span class="math display">\[p(x|y)\sim \mathcal{N}(m|L) \]</span> Apply Bayesian theorem, we have <span class="math display">\[\begin{aligned}p(x|y) &amp;\propto p(y|x)p(x)\\ &amp;\Rightarrow \frac{1}{2}{(x - m)^T}{L^{ - 1} }(x - m) \propto  - \frac{1}{2}{(y - Ax)^T}{O^{ - 1} }(y - Ax) - \frac{1}{2}{(x - \mu )^T}{\Sigma ^{ - 1} }(x - \mu )\\ &amp;\Rightarrow \left\{ \begin{array}{l}{L^{ - 1} } = {A^T}{Q^{ - 1} }A + {\Sigma ^{ - 1} }\\{L^{ - 1} }m = {A^T}{Q^{ - 1} }y + {\Sigma ^{ - 1} }\mu \end{array} \right.\end{aligned} \]</span> The equations are derived by letting the parameters of the quadratic and linear terms in the exponent of the Gaussian distribution equal to each other. By solving the equations, we can get the mean and covariance of the posterior distribution.</p><p>There is a general theorem to calculate the inverse matrix <span class="math display">\[{[A + BCD]^{ - 1} } = {A^{ - 1} } - {A^{ - 1} }B{[{C^{ - 1} } + D{A^{ - 1} }B]^{ - 1} }D{A^{ - 1} } \]</span> By using this theorem, we can easily get the result <span class="math display">\[\left\{ \begin{array}{l}L = (I - KA)\Sigma \\m = \mu  + K(y - A\mu )\end{array} \right. \]</span> in which <span class="math inline">\(K = \Sigma A^T(A\Sigma A^T + Q)^{-1}\)</span> called kalman gain. Thus the posterior distribution is <span class="math display">\[p(x|y)=\mathcal{N}(x|y+K(y-A\mu),(I-K A)\Sigma)\]</span></p><p>By expressing the Gaussian distribution in terms of the precision matrix, we can simplify the calculation of the posterior distribution. Given that <span class="math display">\[\begin{aligned}p({\bf{x} }) = {\cal N}\left( { {\bf{x} }|\mu ,{ {\bf{\Lambda } }^{ - 1} } } \right)\\p({\bf{v} }) = {\cal N}\left( { {\bf{v} }|0,{ {\bf{L} }^{ - 1} } } \right)\end{aligned} \]</span> we have <span class="math display">\[\begin{aligned}&amp;p({\bf{y} }|{\bf{x} }) = {\cal N}\left( { {\bf{y} }|{\bf{Ax} } + {\bf{b} },{ {\bf{L} }^{ - 1} } } \right)\\&amp;p({\bf{y} }) = {\cal N}({\bf{y} }|{\bf{A} }\mu  + {\bf{b} },{ {\bf{L} }^{ - 1} } + {\bf{A} }{\Lambda ^{ - 1} }{ {\bf{A} }^{\rm{T} } })\end{aligned} \]</span> and the posterior distribution is <span class="math display">\[p({\bf{x} }|{\bf{y} }) = {\cal N}({\bf{x} }|{\bf{\Sigma } }\{ { {\bf{A} }^{\rm{T} } }{\bf{L} }({\bf{y} } - {\bf{b} }) + {\bf{A} }\mu \} ,{\bf{\Sigma } }) \]</span> where <span class="math display">\[{\bf{\Sigma } } = {({\bf{\Lambda } } + {\bf{A} }^{\rm{T} }{\bf{LA} })^{ - 1} }\]</span> This expression is much simpler than the previous one not using the inverse matrix theorem</p><h2 id="partition-multivariate-gaussian">Partition Multivariate Gaussian</h2><p>Given a multivariate Gaussian distribution <span class="math display">\[p(\mathbf{x})={\mathcal{N} }(\mathbf{x}|\mu,\mathbf{\boldsymbol{z} })\qquad\Lambda=\Sigma^{-1}\]</span> we can partition the random vector into two parts, say <span class="math display">\[\mathbf{x}=\begin{bmatrix}\mathbf{x}_{a}\\\mathbf{x}_{b}\end{bmatrix}\]</span>, where <span class="math inline">\(\mathbf{x}_{a}\)</span> and <span class="math inline">\(\mathbf{x}_{b}\)</span> are two sub-vectors. The mean, covariance and precision matrix of the original Gaussian distribution become <span class="math display">\[\begin{array}{l}{\bf{\mu } } = \left[ {\begin{array}{c}{ { {\bf{\mu } }_a} }\\{ { {\bf{\mu } }_b} }\end{array} } \right]\\{\bf{\Sigma } } = \left[ {\begin{array}{c}{ { {\bf{\Sigma } }_{aa} } }&amp;{ { {\bf{\Sigma } }_{ab} } }\\{ { {\bf{\Sigma } }_{ba} } }&amp;{ { {\bf{\Sigma } }_{bb} } }\end{array} } \right]\\{\bf{\Lambda } } = \left[ {\begin{array}{c}{ {\Lambda _{aa} } }&amp;{ { {\rm{A} }_{ab} } }\\{ {\Lambda _{ba} } }&amp;{ { {\rm{A} }_{bb} } }\end{array} } \right]\end{array} \]</span> Suppose that there is a relationship between <span class="math inline">\(\mathbf{x}_{a}\)</span> and <span class="math inline">\(\mathbf{x}_{b}, w\sim \mathcal{N}(w|0, \Sigma_w)\)</span> <span class="math display">\[\mathbf{x}_{a}=A\mathbf{x}_{b}+w\]</span> There are some properties of the partitioned Gaussian distribution * The conditional covariance matrix of <span class="math inline">\(\mathbf{x}_{a}\)</span> given <span class="math inline">\(\mathbf{x}_{b}\)</span> is <span class="math display">\[\mathbf{\Sigma}_{a|b}=\mathbf{\Sigma}_{w}\]</span> Since <span class="math inline">\(\mathbf{x}_b\)</span> is given, the covariance of <span class="math inline">\(\mathbf{x}_a\)</span> is the same as the covariance of <span class="math inline">\(w\)</span>. * Calculate the conditional expectations. Do decentalization on the original Gaussian distribution first <span class="math display">\[{E}[x_{a}]={E}[A x_{b}+w]=A{E}[x_{b}]+{E}[w]=A\mu_{b}+0=\mu_{a}\\\Rightarrow \left(x_{a}-\mu_{a}\right)=A(x_{b}-\mu_{b})+w\]</span> To obtain <span class="math inline">\(\mathbf{\mu}_{\mathbf{x}_a|\mathbf{x}_b}\)</span>, we can start from <span class="math inline">\(E[\mathbf{x}_a-\mu_a|\mathbf{x}_b]\)</span> <span class="math display">\[\begin{aligned}E[{x_a} - {\mu _a}|{x_b}] &amp;= E[A({x_b} - {\mu _b}) + w|{x_b}]\\ &amp;= E[A({x_b} - {\mu _b})|{x_b}]\\ &amp;= A({x_b} - {\mu _b})\\ &amp;\Rightarrow E[{x_a}|{x_b}] - {\mu _a} = A({x_b} - {\mu _b})\\ &amp;\Rightarrow {\mu _{ {x_a}|{x_b} } } = A({x_b} - {\mu _b}) + {\mu _a}\end{aligned} \]</span> * Relations of the covariance matrix <span class="math display">\[\begin{aligned}{\Sigma _{ab} }{\mkern 1mu}  &amp;= \text{Cov}({ {\bf{x} }_a},{ {\bf{x} }_b})\\ &amp;= {\mkern 1mu} \text{Cov}(A{ {\bf{x} }_b} + w,{ {\bf{x} }_b})\\ &amp;= {\mkern 1mu} A\text{Cov}({ {\bf{x} }_b},{ {\bf{x} }_b}) + \text{Cov}(w,{ {\bf{x} }_b})\\ &amp;= A{\Sigma _{bb} }\\ &amp;\Rightarrow A = {\Sigma _{ab} }{\Sigma_{bb}^{ - 1} }\end{aligned} \]</span> and <span class="math display">\[\begin{aligned}{\Sigma _{aa} }{\mkern 1mu} &amp; = {\rm{Cov} }({ {\bf{x} }_a},{ {\bf{x} }_a})\\ &amp;= {\mkern 1mu} {\rm{Cov} }(A{ {\bf{x} }_b} + w,A{ {\bf{x} }_b} + w)\\ &amp;= {\rm{Cov} }(A{ {\bf{x} }_b},A{ {\bf{x} }_b}) + {\rm{Cov} }(A{ {\bf{x} }_b},w) + {\rm{Cov} }(w,A{ {\bf{x} }_b}) + {\rm{Cov} }(w,w)\\ &amp;= {\rm{Cov} }(A{ {\bf{x} }_b},A{ {\bf{x} }_b}) + {\rm{Cov} }(w,w)\\ &amp;= A{\rm{Cov} }({ {\bf{x} }_b},{ {\bf{x} }_b}){A^T} + {\rm{Cov} }(w,w)\\ &amp;= A{\Sigma _{bb} }{A^T} + {\Sigma _w}\end{aligned} \]</span> Substitute <span class="math inline">\(A = {\Sigma _{ab} }{\Sigma_{bb}^{ - 1} }\)</span> into the equation above, we have <span class="math display">\[{\Sigma _{aa} } = {\Sigma _{ab} }{\Sigma_{bb}^{ - 1} }{\Sigma _{bb} }{\Sigma_{bb}^{ - 1} }{\Sigma _{ba} } + {\Sigma _w} = {\Sigma _{ab} }{\Sigma_{bb}^{ - 1} }{\Sigma _{ba} } + {\Sigma _{a|b} }\]</span> * From the results above, we can get the conditional distribution of <span class="math inline">\(\mathbf{x}_a\)</span> given <span class="math inline">\(\mathbf{x}_b\)</span> <span class="math display">\[\left\{ \begin{array}{l}{\mu _{a|b} } = {\Sigma _{ab} }\Sigma _{bb}^{ - 1}({x_b} - {\mu _b}) + {\mu _a}\\{\Sigma _{a|b} } = {\Sigma _{aa} } - {\Sigma _{ab} }\Sigma _{bb}^{ - 1}{\Sigma _{ba} }\end{array} \right. \]</span> * Illustration of the partitioned Gaussian distribution. The green line level graph is the joint distribution, while the red line level graph is the conditional distribution and the blue line marginal distribution <img src="image-4.png" alt="alt text" /></p><h2 id="maximum-likelihood-for-gaussian">Maximum Likelihood for Gaussian</h2><ul><li><p>Given i.i.d data <span class="math display">\[\mathbf{X}=\left(\mathbf{x}_{1},\cdot\cdot\cdot,\mathbf{x}_{N}\right)^{\mathrm{T} }\]</span> where <span class="math inline">\(\mathbf{x}_{n}\)</span> is a vector. The likelihood function is <span class="math display">\[\ln p({\bf{X} }|\mu ,{\bf{\Sigma } }) =  - \frac{ {ND} }{2}\ln (2\pi ) - \frac{N}{2}\ln |{\bf{\Sigma } }| - \frac{1}{2}\sum\limits_{n = 1}^N { { {({ {\bf{x} }_n} - \mu )}^{\rm{T} } } } { {\bf{\Sigma } }^{ - 1} }({ {\bf{x} }_n} - \mu ) \]</span> Calculate the derivative and set it to zero, we can get the maximum likelihood estimation of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\Sigma\)</span> <span class="math display">\[\begin{aligned}&amp;\frac{\partial }{ {\partial \mu } }\ln p({\bf{X} }|\mu ,\Sigma ) = \sum\limits_{n = 1}^N { {\Sigma ^{ - 1} } } ({ {\bf{x} }_n} - \mu ) = 0\\ &amp;\Rightarrow {\mu _{ {\rm{ML} } } } = \frac{1}{N}\sum\limits_{n = 1}^N { { {\bf{x} }_n} } \end{aligned} \]</span> and similar for <span class="math inline">\(\Sigma\)</span> <span class="math display">\[{\Sigma _{ {\rm{ML} } } } = \frac{1}{N}\sum\limits_{n = 1}^N {({ {\bf{x} }_n} - {\mu _{ {\rm{ML} } } }){ {({ {\bf{x} }_n} - {\mu _{ {\rm{ML} } } })}^{\rm{T} } } }  \]</span></p></li><li>The maximum likelihood estimation of the covariance matrix is biased. Assume that <span class="math display">\[\mathbf{x}_n=\mu + \epsilon_n\]</span> in which <span class="math display">\[\epsilon_n\sim\mathcal{N}(0,\Sigma)\]</span> Then <span class="math inline">\(\mu_{ML}\)</span> can be rewritten as <span class="math display">\[\begin{aligned}  \mu_{\text{ML} } &amp;= \frac{1}{N} \sum_{n=1}^N \mathbf{x}_n \\  &amp;= \frac{1}{N} \sum_{n=1}^N (\mu + \epsilon_n) \\  &amp;= \mu + \frac{1}{N} \sum_{n=1}^N \epsilon_n\end{aligned}\]</span> Substitute everything abouve into the formula of <span class="math inline">\(\Sigma_{ML}\)</span>, we have <span class="math display">\[\begin{aligned}  \hat{\Sigma}_{\text{ML} } &amp;= \frac{1}{N} \sum_{n=1}^N (x_n - \mu_{\text{ML} })(x_n - \mu_{\text{ML} })^T \\  &amp;= \frac{1}{N} \sum_{n=1}^N \left[(\epsilon_n - \frac{1}{N} \sum_{i=1}^N \epsilon_i)(\epsilon_n - \frac{1}{N} \sum_{i=1}^N \epsilon_i)^T \right] \\  &amp;= \frac{1}{N} \sum_{n=1}^N (\epsilon_n - \bar{\epsilon})(\epsilon_n - \bar{\epsilon})^T \quad \text{in which} \quad \bar{\epsilon} = \frac{1}{N} \sum_{i=1}^N \epsilon_i \end{aligned}   \]</span> Calculate the expectation of <span class="math inline">\(\hat{\Sigma}_{\text{ML} }\)</span> <span class="math display">\[\begin{aligned}  E[\hat{\Sigma}_{\text{ML} }] &amp;= \frac{1}{N} \sum_{n=1}^N {E}\left[(\epsilon_n - \bar{\epsilon})(\epsilon_n - \bar{\epsilon})^T\right] \\  &amp;= \frac{1}{N} \sum_{n=1}^N \left( E[\epsilon_n \epsilon_n^T] - \frac{1}{N}E[\epsilon_n \sum_{i=1}^N \epsilon_i^T] - \frac{1}{N}E\left[\left(\sum_{i=1}^N \epsilon_i\right) \epsilon_n^T\right] + \frac{1}{N^2}E\left[\left(\sum_{i=1}^N \epsilon_i\right)\left(\sum_{j=1}^N \epsilon_j\right)^T\right] \right) \\  &amp;= \frac{1}{N} \left( N \Sigma - \frac{2(N-1)}{N} \Sigma + \frac{N(N-1)}{N^2} \Sigma \right) \\  &amp;= \frac{1}{N} \left( (N - 1)\Sigma \right) \\  &amp;= \frac{N - 1}{N} \Sigma\end{aligned} \]</span> Thus the maximum likelihood estimation of the covariance matrix is biased. The unbiased estimation is <span class="math display">\[{\tilde \Sigma}={\frac{1}{N-1} }\sum_{n=1}^{N}(\mathbf{x}_{n}-\mu_{\mathrm{ML} })(\mathbf{x}_{n}-\mu_{\mathrm{ML} })^{T}\]</span></li><li><p>Sequential Estimation: When the data is given sequentially, we don't need to traverse through all data to calculate the mean value <span class="math display">\[\begin{align*}\mu _{ML}^{(N)} &amp;= \frac{1}{N}\sum\limits_{n = 1}^N { { x_n} } \\ &amp;= \frac{1}{N}{x_N} + \frac{1}{N}\sum\limits_{n = 1}^{N - 1} { { x_n} } \\ &amp;= \frac{1}{N}{x_N} + \frac{ { N - 1} }{N}\mu _{ML}^{(N - 1)}\\ &amp;= \mu _{ML}^{(N - 1)} + \frac{1}{N}({x_N} - \mu _{ML}^{(N - 1)})\end{align*}\]</span> This recursive update rule allows you to efficiently compute the mean as new data points arrive, without needing to recalculate the entire sum each time. It's particularly useful in online learning scenarios where data is processed incrementally</p></li></ul><h2 id="robbins-monro-algorithm">Robbins-Monro Algorithm</h2><ul><li><p>Background: The Robbins-Monro algorithm is a stochastic approximation algorithm that can be used to solve an equation when only noisy measurements of the function are available.</p></li><li><p>Problem: Consider <span class="math inline">\(\theta\)</span> and <span class="math inline">\(z\)</span> which is an r.v. Define a function <span class="math display">\[f(\theta)={E}[z]\theta]=\int z p(z|\theta)\,\mathrm{d}z\]</span> we wanna find the root of <span class="math inline">\(f(\theta^*)=0\)</span>. The query of <span class="math inline">\(z\)</span> is given, one at a time. <img src="image-5.png" alt="alt text" /></p></li><li>Step<ul><li>Initialize <span class="math inline">\(\theta_0\)</span></li><li>For <span class="math inline">\(n=1,2,\cdot\cdot\cdot\)</span><ul><li>Get <span class="math inline">\(z_n\)</span></li><li>Update <span class="math inline">\(\theta_n=\theta_{n-1}-a_nz_n\)</span> here <span class="math inline">\(z_n\)</span> is a noisy measurement, <span class="math inline">\(z_n=h(\theta)+\epsilon _n\)</span></li></ul></li><li>Learning rate: The learning rate <span class="math inline">\(a_n\)</span> decreases over time, ensuring that the step size reduces as the number of iterations increases. This helps the algorithm converge towards the root <span class="math inline">\(ùúÉ\)</span>. Frequently used learning rate is<span class="math display">\[{a_n} = \;\frac{1}{ { {n^\gamma } } },\gamma  \in (0.5,1]\]</span></li><li>The learning rate should satisfy the three conditions below to ensure the convergence of the algorithm <span class="math display">\[\operatorname*{lim}_{N\rightarrow\infty}a_{N}=0\qquad\sum_{N=1}^{\infty}a_{N}=\infty\qquad\sum_{N=1}^{\infty}a_{N}^{2}\lt \infty\]</span></li></ul></li><li>Use Robins-Monro algorithm in maximum likelihood estimation<ul><li>We wanna find <span class="math inline">\(\theta^*\)</span> s.t. <span class="math display">\[f(\theta ) =  - \mathop {\lim }\limits_{N \to \infty } \frac{1}{N}\sum\limits_{n = 1}^N {\frac{\partial }{ { \partial \theta } } } \ln p({x_n}|\theta ) = {E_x}\left[ { - \frac{\partial }{ { \partial \theta } }\ln p(x|\theta )} \right] = 0\]</span> This is the objective function</li><li>Thus, the update step is <span class="math display">\[\theta^{(N)}=\theta^{(N-1)}-a_{N-1}\frac{\partial}{\partial\theta^{(N-1)} }\left[-\ln p(x_{N}|\theta^{(N-1)})\right]\]</span></li></ul></li><li>Example: Use Robin-Monro Algorithm to estimate the mean value of Gaussion.<ul><li>The gradient in the update equation is <span class="math display">\[z=\frac{\partial}{\partial\mu_{M L} }\left[-\ln p(x|\mu_{M L},\sigma^{2})\right]=-\frac{1}{\sigma^{2} }(x-\mu_{M L})\]</span></li><li>The dynamic learning rate is set to be <span class="math inline">\(a_{N}=\,\sigma^{2}/N\)</span></li><li>Substitute the parameters into the update equation, we have<span class="math display">\[\mu_{ML}^{(N)} = \mu_{ML}^{(N-1)} + \frac{\sigma^2}{N} \cdot (x_N - \mu_{ML}^{(N-1)})\]</span></li><li>As shown in the graph, if the <span class="math inline">\(\mu_{ML}\)</span> deviate from the true value, the update will be larger. <img src="image-6.png" alt="alt text" /></li></ul></li></ul><h2 id="bayesian-inference-for-gaussian">Bayesian Inference for Gaussian</h2><ul><li>Here assume that <span class="math inline">\(\sigma^2\)</span> is known, and we need to estimate <span class="math inline">\(\mu\)</span>. The prior distribution is <span class="math inline">\(\mu\sim\mathcal{N}(\mu_0,\sigma_0^2)\)</span>, the likelihood function is <span class="math display">\[p({\bf x}|\mu)=\prod_{n=1}^{N}p(x_{n}|\mu)=\frac{1}{(2\pi\sigma^{2})^{N/2} }\exp\left\{-\frac{1}{2\sigma^{2} }\sum_{n=1}^{N}(x_{n}-\mu)^{2}\right\}\]</span> Apply Bayes' theorem, and assume a distribution for the posterior distribution, we have<span class="math display">\[\begin{align*}&amp;p(\mu |{\bf{x} }) \propto p({\bf{x} }|\mu )p(\mu )\\&amp;p(\mu |{\bf{x} })\sim \mathcal{N}\left( {\mu |{\mu _N},{\mkern 1mu} \sigma _N^2} \right)\end{align*}\]</span> By comparing the corresponding terms, we can get the parameters in posterior distribution <span class="math display">\[\begin{align*}&amp;{\mu _N} = \frac{ { {\sigma ^2} } }{ { N\sigma _0^2 + {\sigma ^2} } }{\mu _0} + \frac{ { N\sigma _0^2} }{ { N\sigma _0^2 + {\sigma ^2} } }{\mu _{ { \rm{ML} } } }\\&amp;\frac{1}{ { \sigma _N^2} }\;\; = \;\;\frac{1}{ { \sigma _0^2} } + \frac{N}{ { {\sigma ^2} } }\end{align*}\]</span> As <span class="math inline">\(N\)</span>, the number of samples vary, the posterior distribution will converge to the maximum likelihood estimation. The prior distribution will have less influence on the posterior distribution.</li></ul><table><thead><tr class="header"><th></th><th>N = 0</th><th>N ‚Üí ‚àû</th></tr></thead><tbody><tr class="odd"><td>( _N )</td><td>( _0 )</td><td>( _{ML} )</td></tr><tr class="even"><td>( ^2_N )</td><td>( ^2_0 )</td><td>0</td></tr></tbody></table><ul><li>Sequential Estimation Using Bayesian Inference: The core principle is to use the posterior distribution of the previous <span class="math inline">\(N-1\)</span> step as the prior distribution of the <span class="math inline">\(N\)</span>'th step. The original form is still about Bayes' theorem. <span class="math inline">\(p(\mu)\)</span> is the prior, the consecutive multiplication is likelihood<span class="math display">\[p(\mu |x) \propto p(\mu )p({x_N}|\mu )\prod\limits_{n = 1}^{N - 1} p ({x_n}|\mu )\]</span> Since the posterior of previous <span class="math inline">\(N-1\)</span> step satisfies <span class="math display">\[p(\mu |{x_1},{x_2}, \ldots ,{x_{N - 1} })\propto p(\mu )\prod\limits_{n = 1}^{N - 1} p ({x_n}|\mu )\]</span> and the posterior of previous <span class="math inline">\(N-1\)</span> steps are used as the prior of step <span class="math inline">\(N\)</span>, thus the formula above can be simplified as <span class="math display">\[\begin{align*}p(\mu |{x_1},{x_2}, \ldots ,{x_N}) &amp;\propto p({x_N}|\mu ) \cdot p(\mu |{x_1},{x_2}, \ldots ,{x_{N - 1} })\\ &amp;\propto {\cal N}\left( {\mu |{\mu _{N - 1} },\sigma _{N - 1}^2} \right)p({x_N}|\mu )\end{align*}\]</span></li><li>Here assume that <span class="math inline">\(\mu\)</span> is known and we wanna estimate the precision <span class="math inline">\(\lambda = \frac{1}{\sigma^2}\)</span>. The likelihood function is given by <span class="math display">\[p(\mathbf{x}|\lambda)=\prod_{n=1}^{N}{\mathcal{N} }(x_{n}|\mu,\lambda^{-1})\propto\lambda^{N/2}\exp\left\{-{\frac{\lambda}{2} }\sum_{n=1}^{N}(x_{n}-\mu)^{2}\right\}\]</span> Actually the likelihood function is a Gamma distribution of <span class="math inline">\(\lambda\)</span>, which is defined as <span class="math display">\[\mathrm{Gam}(\lambda|a,b)=\frac{1}{\Gamma(a)}b^{a}\lambda^{a-1}\exp(-b\lambda)\]</span> the expectation and variance of the Gamma distribution are <span class="math display">\[E[\lambda ] = \frac{a}{b},{\mathop{\rm var} } [\lambda ] = \frac{a}{ { {b^2} } }\]</span> Now assume a prior distribution for <span class="math inline">\(\lambda\)</span> is <span class="math inline">\(\lambda\sim\mathrm{Gam}(\lambda|a_0,b_0)\)</span>, the posterior distribution is <span class="math display">\[p(\lambda|{\bf x})\propto\lambda^{a_{0}-1}\lambda^{N/2}\exp\left\{-b_{0}\lambda-\frac{\lambda}{2}\sum_{n=1}^{N}(x_{n}-\mu)^{2}\right\}\]</span> The paremeters are <span class="math display">\[\begin{align*}&amp;{a_N} = {a_0} + \frac{N}{2}\\&amp;{b_N} = {b_0} + \frac{1}{2}\sum\limits_{n = 1}^N { { {({x_n} - \mu )}^2} }  = {b_0} + \frac{N}{2}\sigma _{ {\rm{ML} } }^2\end{align*}\]</span></li><li>If both <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\lambda\)</span> are unknown, the likelihood function is</li></ul><p><span class="math display">\[\begin{align*}p({\bf{x} }|\mu ,\lambda ) &amp;= \prod\limits_{n = 1}^N { { {\left( {\frac{\lambda }{ {2\pi } } } \right)}^{1/2} } } \exp \left\{ { - \frac{\lambda }{2}{ {({x_n} - \mu )}^2} } \right\}\\ &amp;\propto {\left[ { {\lambda ^{1/2} }\exp \left( { - \frac{ {\lambda {\mu ^2} } }{2} } \right)} \right]^N}\exp \left\{ {\lambda \mu \sum\limits_{n = 1}^N { {x_n} }  - \frac{\lambda }{2}\sum\limits_{n = 1}^N {x_n^2} } \right\}\end{align*}\]</span> To have a conjugate prior of the likelihood function, we introduce Gaussian-gamma distribution here. The definition is derived from the product of Gaussian and Gamma distribution <span class="math display">\[\begin{align*}\mu |\lambda &amp;\sim {\cal N}({\mu _0},{\mkern 1mu} {(\beta \lambda )^{ - 1} }) \Rightarrow p(\mu |\lambda ) = \sqrt {\frac{\lambda }{ {2\pi \beta } } } \exp \left( { - \frac{ {\lambda { {(\mu  - {\mu _0})}^2} } }{ {2\beta } } } \right)\\ \lambda &amp;\sim Gamma(\alpha ,\beta ) \Rightarrow p(\lambda ) = \frac{ { {\beta ^\alpha } } }{ {\Gamma (\alpha )} }{\lambda ^{\alpha  - 1} }\exp ( - \beta \lambda )\end{align*}\]</span> which eventually defined as <span class="math display">\[p(\mu,\lambda)=\sqrt{\frac{\lambda}{2\pi\beta} }\exp\left(-\frac{\lambda(\mu-\mu_{0})^{2} }{2\beta}\right)\cdot\frac{\beta^{\alpha} }{\Gamma(\alpha)}\lambda^{\alpha-1}\exp(-\beta\lambda)\]</span> This distribution combines both the prior distribution of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\lambda\)</span>. * If <span class="math inline">\(\mu\)</span> is known, <span class="math inline">\(\lambda\)</span> follows a Gamma distribution * If <span class="math inline">\(\lambda\)</span> is known, <span class="math inline">\(\mu\)</span> follows a Gaussian distribution</p><p>Thus the prior distribution we use here is <span class="math display">\[p(\mu ,\lambda ) \propto {\left[ { {\lambda ^{1/2} }\exp \left( { - \frac{ {\lambda {\mu ^2} } }{2} } \right)} \right]^\beta }\exp \left\{ {c\lambda \mu  - d\lambda } \right\}\]</span> which has a similar form as the likelihood function. The process below is to show that the prior has an aligned form with the Gaussian-Gamma distribution <span class="math display">\[\begin{align*}p(\mu ,\lambda ) &amp;\propto { {\left[ { {\lambda ^{1/2} }\exp \left( { - \frac{ {\lambda {\mu ^2} } }{2} } \right)} \right]}^\beta }\exp \left\{ {c\lambda \mu  - d\lambda } \right\}\\&amp;\Rightarrow p(\mu ,\lambda ) \propto {\lambda ^{\beta /2} }\exp \left( { - \frac{ {\beta \lambda {\mu ^2} } }{2} + c\lambda \mu  - d\lambda } \right)\\&amp;\Rightarrow p(\mu ,\lambda ) \propto {\lambda ^{\beta /2} }\exp \left( { - \frac{ {\beta \lambda } }{2}\left( { {\mu ^2} - \frac{ {2c\mu } }{\beta } } \right) - d\lambda } \right)\\&amp;\Rightarrow p(\mu ,\lambda ) \propto {\lambda ^{\beta /2} }\exp \left( { - \frac{ {\beta \lambda } }{2}\left( { { {\left( {\mu  - \frac{c}{\beta } } \right)}^2} - \frac{ { {c^2} } }{ { {\beta ^2} } } } \right) - d\lambda } \right)\\&amp;\Rightarrow p(\mu ,\lambda ) \propto {\lambda ^{\beta /2} }\exp \left( { - \frac{ {\beta \lambda } }{2}{ {\left( {\mu  - \frac{c}{\beta } } \right)}^2} + \frac{ {\lambda {c^2} } }{ {2\beta } } - d\lambda } \right)\\&amp;\Rightarrow p(\mu ,\lambda ) \propto {\lambda ^{\beta /2} }\exp \left( { - \frac{ {\beta \lambda } }{2}{ {\left( {\mu  - \frac{c}{\beta } } \right)}^2} } \right)\exp \left( { - \left( {d - \frac{ { {c^2} } }{ {2\beta } } } \right)\lambda } \right)\\&amp;\Rightarrow p(\mu ,\lambda ) = \exp \left\{ { - \frac{ {\beta \lambda } }{2}{ {(\mu  - c/\beta )}^2} } \right\}{\lambda ^{\beta /2} }\exp \left\{ { - \left( {d - \frac{ { {c^2} } }{ {2\beta } } } \right)\lambda } \right\}\end{align*}\]</span> The posterior distribution is still a Gaussian-gamma distribution with parameters <span class="math display">\[{\beta _N} = \beta  + N,\quad {c_N} = c + \sum\limits_{n = 1}^N { {x_N} } ,\quad {d_N} = d + \frac{1}{2}\sum\limits_{n = 1}^N {x_N^2}\]</span> * In terms of maltivariate Gaussian, there is some difference * <strong>Œº unknown, Œõ known</strong>: ( p() ) Gaussian.</p><ul><li><p><strong>Œõ unknown, Œº known</strong>: ( p() ) Wishart,<br /><span class="math display">\[  \mathcal{W}(\Lambda | \mathbf{W}, \nu) = B |\Lambda|^{(\nu - D - 1)/2} \exp\left( -\frac{1}{2} \text{Tr}(\mathbf{W}^{-1} \Lambda) \right)  \]</span></p></li><li><p><strong>Œõ and Œº unknown</strong>: ( p(, ) ) Gaussian-Wishart,<br /><span class="math display">\[  p(\mu, \Lambda | \mu_0, \beta, \mathbf{W}, \nu) = \mathcal{N}(\mu | \mu_0, (\beta \Lambda)^{-1}) \mathcal{W}(\Lambda | \mathbf{W}, \nu)  \]</span></p></li></ul><h2 id="gaussian-mixture-modelgmm">Gaussian Mixture Model(GMM)</h2><p><img src="image-7.png" alt="alt text" /> The left plot shows a single Gaussian model where all the data points (green dots) are modeled using one Gaussian distribution. The blue contours represent the levels of the Gaussian distribution. This model works well if the data forms one concentrated group, but it's limited when there are multiple groups within the data.</p><p>The right plot shows a mixture of two Gaussian distributions, where the data is assumed to come from two different Gaussians. The contours represent the two Gaussian components, and the data points form two distinct clusters. This mixture model can capture the underlying structure of more complex datasets with multiple groups.</p><p>The probability density function of a GMM is defined as <span class="math display">\[p({\bf x})\equiv\sum_{k=1}^{K}\pi_{k}\mathcal{N}({\bf x}|\mu_{k},\Sigma_{k})\]</span> where <span class="math inline">\(K\)</span> is the number of Gaussian clusters, <span class="math inline">\(\pi_{k}\)</span> is the mixing coefficient, <span class="math inline">\(\mu_{k}\)</span> is the mean, and <span class="math inline">\(\Sigma_{k}\)</span> is the covariance matrix of the <span class="math inline">\(k\)</span>'th Gaussian component. The sum of the mixing coefficients is equal to one, <span class="math inline">\(\sum_{k=1}^{K}\pi_{k}=1\)</span>, and <span class="math inline">\(\pi_k\)</span> is non-negative. To estimate the parameters of the GMM, we can use the Expectation-Maximization (EM) algorithm.</p><p>This posterior is referred to as the responsibility <span class="math inline">\(\gamma_{k}\left(\mathbf{x}\right)\)</span>, and is calculated as <span class="math display">\[\begin{align*}{\gamma _k}({\bf{x} }) &amp;= p(k|{\bf{x} })\\ &amp;= \frac{ {p(k)p({\bf{x} }|k)} }{ {\sum\nolimits_l {p(l)p({\bf{x} }|l)} } }\\ &amp;= \frac{ { {\pi _k}N({\bf{x} }|{\mu _k},{\Sigma _k})} }{ {\sum\nolimits_l { {\pi _l}N({\bf{x} }|{\mu _l},{\Sigma _l})} } }\end{align*}\]</span></p><h2 id="students-t-distribution">Student's t-distribution</h2><p>The Student's t-distribution is a probability distribution that is symmetric and bell-shaped, similar to the Gaussian distribution, but has heavier tails. It is defined as <span class="math display">\[\begin{align*}p(x | \mu, a, b) &amp;= \int_{0}^{\infty} \mathcal{N}(x | \mu, \tau^{-1}) \, \text{Gam}(\tau | a, b) \, d\tau \\&amp;= \int_{0}^{\infty} \mathcal{N}(x | \mu, (\eta \lambda)^{-1}) \, \text{Gam}(\eta | \nu/2, \nu/2) \, d\eta \\&amp;= \frac{\Gamma(\nu/2 + 1/2)}{\Gamma(\nu/2)} \left( \frac{\lambda}{\pi \nu} \right)^{1/2} \left[ 1 + \frac{\lambda (x - \mu)^2}{\nu} \right]^{-\nu/2 - 1/2} \\&amp;= \text{St}(x | \mu, \lambda, \nu)\end{align*}\]</span> The paremeters are defined as <span class="math display">\[\begin{align*}\lambda &amp;= \frac{a}{b} \\\eta &amp;= \frac{\tau b}{a} \\\nu &amp;= 2a\end{align*}\]</span> The Student's t-distribution is infinite mixture of Gaussians, with the same mean but different variances. The shape of this distribution and its connection with other distributions are shown in the graph below. <img src="image-8.png" alt="alt text" /> When fitting a distribution, the Student's t-distribution, which is infinite mixture of Gaussian, is more robust to outliers than a single Gaussian distribution. Student's t-distribution has heavier tails, which means that it assigns more probability to data points further away from the mean. This property makes it more suitable for modeling data with outliers or heavy tails. <img src="image-9.png" alt="alt text" /> The multivariate Student's t-distribution is defined as <span class="math display">\[\begin{align*}\text{St}(\mathbf{x} | \mu, \mathbf{\Lambda}, \nu) &amp;= \int_{0}^{\infty} \mathcal{N}(\mathbf{x} | \mu, (\eta \mathbf{\Lambda})^{-1}) \, \text{Gam}(\eta | \nu / 2, \nu / 2) \, d\eta \\&amp;= \frac{\Gamma(D / 2 + \nu / 2)}{\Gamma(\nu / 2)} \frac{|\mathbf{\Lambda}|^{1/2} }{(\pi \nu)^{D / 2} } \left[ 1 + \frac{\Delta^2}{\nu} \right]^{-D / 2 - \nu / 2}\end{align*}\]</span> in which <span class="math display">\[\Delta^2 = (\mathbf{x} - \mu)^\top \mathbf{\Lambda} (\mathbf{x} - \mu)\]</span> The properties <span class="math display">\[\begin{align*}\mathbb{E}[\mathbf{x}] &amp;= \mu, \quad \text{if } \nu &gt; 1 \\\text{cov}[\mathbf{x}] &amp;= \frac{\nu}{\nu - 2} \mathbf{\Lambda}^{-1}, \quad \text{if } \nu &gt; 2 \\\text{mode}[\mathbf{x}] &amp;= \mu\end{align*}\]</span></p><h2 id="von-mises-distribution">Von Mises Distribution</h2><ul><li>Periodic random variable is a random variable whose values repeat at regular intervals, formally satisfying <span class="math display">\[  \begin{align*}      p(\theta) &amp;\geq 0 \\      \int_{0}^{2\pi} p(\theta) \, d\theta &amp;= 1 \\      p(\theta + 2\pi) &amp;= p(\theta).  \end{align*}\]</span></li><li><p>The Von Mises distribution is a circular version of the Gaussian distribution, defined as <span class="math display">\[\begin{align*}  p(\theta | \theta_0, m) &amp;= \frac{1}{2\pi I_0(m)} \exp \{ m \cos(\theta - \theta_0) \} \\  \text{where} \quad I_0(m) &amp;= \frac{1}{2\pi} \int_{0}^{2\pi} \exp \{ m \cos \theta \} \, d\theta\end{align*}\]</span> <span class="math inline">\(\theta_0\)</span> here is the expectation of the distribution, and <span class="math inline">\(m\)</span> is the concentration parameter. The concentration parameter <span class="math inline">\(m\)</span> controls the spread of the distribution, with higher values of <span class="math inline">\(m\)</span> leading to a more concentrated distribution. <img src="image-10.png" alt="alt text" /></p></li><li><p>The likelihood function of the Von Mises distribution is <span class="math display">\[\ln p(\mathcal{D} | \theta_0, m) = -N \ln(2\pi) - N \ln I_0(m) + m \sum_{n=1}^{N} \cos(\theta_n - \theta_0).\]</span> maximize the likelihood function to estimate the parameters <span class="math inline">\(\theta_0\)</span> and <span class="math inline">\(m\)</span> <span class="math display">\[\begin{align*}\theta _0^{ML} &amp;= {\tan ^{ - 1} }\left( {\frac{ {\sum\limits_n {\sin } {\theta _n} } }{ {\sum\limits_n {\cos } {\theta _n} } } } \right)\\\frac{ { {I_1}({m_{ML} })} }{ { {I_0}({m_{ML} })} } &amp;= \frac{1}{N}\sum\limits_{n = 1}^N {\cos } ({\theta _n} - \theta _0^{ML}),\end{align*}\]</span> <span class="math inline">\(m_{ML}\)</span> can be obtained through numerical methods.</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch Distributed Training(1)--DP/DDP</title>
      <link href="/2024/10/12/Pytorch%20Distributed%20Training(1)--DP_DDP/"/>
      <url>/2024/10/12/Pytorch%20Distributed%20Training(1)--DP_DDP/</url>
      
        <content type="html"><![CDATA[<h2 id="overview">Overview</h2><p>Basically, there are two types of distributed training: data parallelism and model parallelism.</p><ul><li>Data Parallelism: the model is replicated on each device and each replica processes a different portion of the input data. The gradients are then aggregated across all devices and the model is updated.</li><li>Model Parallelism: different parts of the model are placed on different devices and the forward and backward passes are executed in parallel.</li></ul><p>Note that the parallelism in PyTorch is only supported on Linux.</p><h2 id="data-parallelism">Data Parallelism</h2><h3 id="data-paralleldp">Data Parallel(DP)</h3><p>DP is the simplest way to use multiple GPUs. Only one extra line of code is needed to implement DP.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = torch.nn.DataParallel(model)</span><br></pre></td></tr></table></figure><p>The steps are as follows:</p><ul><li><strong>Forward pass</strong>:<ul><li>Scatter a batch of input data from one assigned GPU to all GPUs. <img src="image-6.png" alt="alt text" /></li><li>Replicate the model on all GPUs. <img src="image-7.png" alt="alt text" /></li><li>Parallel forward pass on all GPUs. <img src="image-8.png" alt="alt text" /></li><li>Gather the output from all GPUs to the assigned GPU. <img src="image-9.png" alt="alt text" /></li></ul></li><li><strong>Backward pass</strong>:<ul><li>Compute the loss and gradient on the assigned GPU. <img src="image-10.png" alt="alt text" /></li><li>Scatter the gradient from the assigned GPU to all GPUs. <img src="image-11.png" alt="alt text" /></li><li>Parallel backward pass on all GPUs <img src="image-12.png" alt="alt text" /></li><li>Gather the gradient from all GPUs to the assigned GPU and update the model. <img src="image-13.png" alt="alt text" /></li></ul></li></ul><p>Note that the assigned GPU has to accommodate the whole batch of data, and conduct all the work of model updating. Thus the performance of DP is limited concerning both space and time.</p><h3 id="distributed-data-parallelddp">Distributed Data Parallel(DDP)</h3><h4 id="steps">Steps</h4><p>DDP is a more advanced way to use multiple GPUs. It is more flexible and efficient than DP. The steps are as follows:</p><ul><li><strong>Preparation</strong><ul><li>Replicate the model on all GPUs and divide the input data among all GPUs equally and randomly. Each GPU load its own data from the disk.</li></ul></li><li><strong>Training</strong><ul><li>Forward: The computation of loss function is done on each GPU without gathering the results on one assigned GPU.</li><li>Backward: Each process communicates with each other by All-Reduce operation to exchange the gradients and compute the average gradient. The model is updated on each GPU using the same average gradient.</li><li>Updating: Each process has its own optimizer and updates the model on its own GPU. Since the initial value and the gradient are the same, the model on each GPU is also the same.</li></ul></li></ul><p>To further improve the performance, DDP uses a further optimized version of All-Reduce. Obviously, it is not efficient to communicate after finishing all the computation of gradient. Therefore, the model parameters are partitioned into a lot of buckets, Once the gradient computation in bucket <span class="math inline">\(h-1\)</span> is finished, the communication of gradient in bucket <span class="math inline">\(h-1\)</span> and the gradient computation of bucket <span class="math inline">\(h\)</span> begin simultaneously</p><h4 id="backend-communication">Backend Communication</h4><p>The backend communication of DDP supports diffenrent protocals. The choice of protocals are determined by the factors below</p><ul><li><strong>Network Environment</strong><ul><li>Ethernet: <code>nccl</code> has a better performance, <code>gloo</code> is for spare use</li><li>InfiniBand: <code>nccl</code> only</li></ul></li><li><strong>Operators</strong>: <code>nccl</code> supports more diverse operators than <code>gloo</code></li></ul><p>In practical use, <code>nccl</code> is the prior choice.</p><h4 id="initialization">Initialization</h4><ul><li><strong>TCP mode</strong>: Independently start each process in bash and allocate rank.</li><li><strong>ENV mode</strong>: The program can search for required values in required variables</li></ul><h4 id="start-up">Start Up</h4><ul><li><code>mp.spawn()</code>: The module <code>mp</code> capsulates the package <code>multiprocessing</code>, not specifically designed for DDP</li><li><code>torchrun</code>: Automatically set the configuration of env variables. Only need to set <code>os.environ['CUDA_VISIBLE_DEVICES']</code> manually</li><li><code>torch.distributed.launch</code>: To be deprecated</li></ul><p>Parameters used in the last two ways of startup(i.e., parameters passed through the startup command)</p><ul><li><code>--nproc_per_node</code>: number of processes per machine</li><li><code>--nnodes</code>: number of machines/nodes</li><li><code>--node_rank</code>: the idx of this machine</li><li><code>--master_addr</code>: the IP of idx-0 machine</li><li><code>--master_port</code>: the available port of idx-0 machine</li></ul><p>Other parameters and concepts that may be used in the program</p><ul><li><code>node</code>: a machine, which may contain multiple GPUs</li><li><code>world_size</code>: number of processes in all nodes, ranging from 1 to <code>world_size</code><span class="math inline">\(-1\)</span></li><li><code>local_rank</code>: the rank of the process in the current node. If there are 4 GPUs in one node, the <code>local_rank</code> ranges from 0 to 3</li><li><code>rank</code>: the rank of the process in all nodes</li></ul><h3 id="program-revisionmp.spawn">Program Revision(mp.spawn())</h3><h4 id="import">Import</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import torch.distributed as dist</span><br><span class="line">import torch.multiprocessing as mp</span><br><span class="line">from torch.cuda.amp import GradScaler</span><br><span class="line">from torch.utils.data.distributed import DistributedSampler</span><br><span class="line">from torch.nn.parallel import DistributedDataParallel as DDP</span><br></pre></td></tr></table></figure><ul><li><code>dist</code>: communication between GPUs</li><li><code>mp</code>: DDP startup</li><li><code>GradScaler</code>: automatic mixed precision</li><li><code>DistributedSampler</code>: Data sampler in a distributed setting</li><li><code>DDP</code>: model capsulation and passing</li></ul><h4 id="key-functions">Key functions</h4><ul><li>Function <code>init_ddp</code> for initialization <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def init_ddp(local_rank):</span><br><span class="line">    torch.cuda.set_device(local_rankÔºâ</span><br><span class="line">    os.environ[&#x27;RANK&#x27;] = str(local_rank)</span><br><span class="line">    dist.init_processgroup(backend=&#x27;nccl&#x27;,init_method=&#x27;env://&#x27;)</span><br></pre></td></tr></table></figure> Initialize the process using <code>nccl</code> as communication protocal and ENV mode for initialization. After initializing with <code>init_ddp</code>, we can get <code>local_rank</code> and <code>world_size</code> in following code easily. <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">local_rank = dist.get_rank() # rank of current process</span><br><span class="line">world_size = dist.get_world_size() # number of processes in current server</span><br></pre></td></tr></table></figure> Given that all processes have the same model, Only one processes is responsible for printing log and saving checkpoint. <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">if local_rank == 0:</span><br><span class="line">    print(f&#x27;Begin validating&#x27;)</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure></li><li>Function <code>reduce_tensor</code> to gather data from different processes <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def reduce_tensor(tensor: torch.Tensor):</span><br><span class="line">    rt = tensor.clone()</span><br><span class="line">    dist.all_reduce(rt, op=dist.reduce_op.SUM)</span><br><span class="line">    rt /= dist.get_worldsize)</span><br><span class="line">    return rt</span><br></pre></td></tr></table></figure></li><li>Function <code>get_ddp_generator</code> to enhance the randomness <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def get_ddp_generator(seed=3407):</span><br><span class="line">  local_rank = dist.get_rank()</span><br><span class="line">  g = torch.Generator()</span><br><span class="line">  g.manual_seed(seed + local_rank)                       </span><br><span class="line">  return g</span><br></pre></td></tr></table></figure></li></ul><h4 id="program-entry">Program Entry</h4><ul><li>The parameter <code>nprocs</code> should be equal to <code>worldsize</code> otherwise will cause dead lock <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(&#x27;-gpu&#x27;, default=&#x27;0,1,2,3&#x27;, type=str)</span><br><span class="line">    ...</span><br><span class="line">    os.environ[&#x27;MASTER_ADDR&#x27;] = &#x27;localhost&#x27;</span><br><span class="line">    os.environ[&#x27;MASTER_PORT&#x27;] = &#x27;19198&#x27; [&#x27;CUDA_VISIBLE_DEVICES&#x27;] = args.gpu</span><br><span class="line">    world_size = torch.cuda.device_count() </span><br><span class="line">    os.environ[&#x27;WORLD_SIZE&#x27;] = str(world_size)</span><br><span class="line">    os.environ[&#x27;PYTORCH_CUDA_ALLOC_CONF&#x27;] = &quot;max_split_size_mb:128&quot;</span><br><span class="line"></span><br><span class="line">    if args.mode == &#x27;train&#x27;:</span><br><span class="line">        mp.spawn(fn=train, nprocs=world_size, args=(args,))</span><br></pre></td></tr></table></figure></li></ul><h4 id="main-function">Main Function</h4><ul><li>The argument <code>local_rank</code> is automatically allocated by <code>mp.spawn()</code></li><li>The function <code>init_ddp()</code> is needed for the initialization of every process <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def main(local_rank, args):</span><br><span class="line">    ...</span><br><span class="line">    init_ddp(local_rank)</span><br><span class="line">    ...</span><br><span class="line">    model.cuda()</span><br><span class="line">    model = nn.SyncBatchnorm.convert_sync_batchnorm(model)</span><br><span class="line">    ...</span><br><span class="line">    num_gpus = torch.cuda.device_count()</span><br><span class="line">    if num_gpus &gt; 1: </span><br><span class="line">        model = nn.parallel.DistributedDataParallel(model, device_id=[local_rank], output_device=local_rank)</span><br><span class="line">    ...</span><br><span class="line">    scaler = GradScaler()</span><br><span class="line">    ...</span><br><span class="line">    for epoch in range(epochs):</span><br><span class="line">        if local_rank == 0:</span><br><span class="line">            ....</span><br><span class="line">        train_dataloader.sampler.set_epoch(epoch)</span><br><span class="line">        train(model, ..., scaler, args)</span><br><span class="line">    dist.destroy_process_group()</span><br></pre></td></tr></table></figure></li></ul><h4 id="get_dataloader-function">Get_dataloader Function</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">def get_dataloader(path, args, ..., train: bool):</span><br><span class="line">    ...</span><br><span class="line">    if train:</span><br><span class="line">        train_sampler = DistributedSampler(data, shuffle=True)  </span><br><span class="line">        g = get_ddp_generator()</span><br><span class="line">        dataloader = DataLoader(dataset=data,</span><br><span class="line">                                batch_size=args[&#x27;batch_size&#x27;],</span><br><span class="line">                                num_workers=args[&#x27;num_workers&#x27;],</span><br><span class="line">                                pin_memory=True,</span><br><span class="line">                                shuffle=False, </span><br><span class="line">                                sampler=train_sampler,</span><br><span class="line">                                generator=g)</span><br><span class="line">    else:</span><br><span class="line">        test_sampler = DistributedSampler(data, shuffle=False)  </span><br><span class="line">        dataloader = DataLoader(dataset=data,</span><br><span class="line">                                batch_size=args[&#x27;batch_size&#x27;],</span><br><span class="line">                                num_workers=args[&#x27;num_workers&#x27;],</span><br><span class="line">                                pin_memory=True,</span><br><span class="line">                                shuffle=False,  # ÈááÁî®È°∫Â∫èÈááÊ†∑Âô®</span><br><span class="line">                                sampler=test_sampler)</span><br><span class="line">    </span><br><span class="line">    return dataloader</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="train-function">Train Function</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">def train(...):</span><br><span class="line">    model.train()</span><br><span class="line">    ...</span><br><span class="line">    for step, batch in enumerate(train_dataloader):</span><br><span class="line">        ...</span><br><span class="line">        with torch.cuda.amp.autocast():  </span><br><span class="line">            output = ...</span><br><span class="line">            loss = ...</span><br><span class="line">        ...</span><br><span class="line">        reduced_loss = reduce_tensor(loss.data)  </span><br><span class="line">        if dist.get_rank() == 0:  </span><br><span class="line">            print(...)</span><br><span class="line">        train_loss += reduced_loss.item()</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        scaler.scale(loss).backward()  </span><br><span class="line">        scaler.step(optimizer)  </span><br><span class="line">        scheduler.step()  </span><br><span class="line">        scaler.update()  </span><br><span class="line">        ...</span><br><span class="line">        torch.cuda.empty_cache()  </span><br><span class="line"></span><br><span class="line">    if dist.get_rank() == 0:</span><br><span class="line">        print(...)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h4 id="validation-function">Validation Function</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">@torch.no_grad()</span><br><span class="line">def validate(...):</span><br><span class="line">    model.eval()</span><br><span class="line">    ...</span><br><span class="line">    with torch.cuda.amp.autocast():</span><br><span class="line">        output = ...</span><br><span class="line">        </span><br><span class="line">    loss = ...</span><br><span class="line">    reduced_loss = reduce_tensor(loss.data)</span><br><span class="line">    eval_loss += reduced_loss.item()</span><br><span class="line"></span><br><span class="line">    if dist.get_rank() == 0:</span><br><span class="line">        print(...)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    pred_labels = ...</span><br><span class="line">    true_labels = ...</span><br><span class="line">    pred_bools = ...</span><br><span class="line">    macro = ...</span><br><span class="line"></span><br><span class="line">    macro = reduce_tensor(torch.tensor(macro)).cuda()</span><br><span class="line"></span><br><span class="line">return macro</span><br></pre></td></tr></table></figure><h3 id="program-revisiontorchrun">Program Revision(torchrun)</h3><ul><li>Similar to the previous section</li><li>No need to import <code>mp</code> and use the capsulation of <code>mp.spawn()</code></li><li>Only one env variable needs to be configured in the program</li><li><code>local_rank</code> no more serves as an argument in <code>main</code> function. Call <code>os.environ['LOCAL_RANK']</code> to use the variable</li></ul>]]></content>
      
      
      <categories>
          
          <category> Python Programming </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Series of Attention Mechanisms</title>
      <link href="/2024/10/08/A%20Series%20of%20Attention%20Mechanisms/"/>
      <url>/2024/10/08/A%20Series%20of%20Attention%20Mechanisms/</url>
      
        <content type="html"><![CDATA[<h2 id="attention-is-all-you-need">Attention is all you need</h2><h3 id="intro-background">Intro &amp; Background</h3><p>The intro section first talks about RNN-like models and their drawbacks concerning both time and space</p><ul><li>Since RNN-like models have a inherently sequential nature, i.e., <span class="math inline">\(h_t\)</span> is dependent on the hidden state at time <span class="math inline">\(h_{t-1}\)</span>, they are not parallelizable and hence time-consuming.</li><li>RNN-like models also have to store the hidden states of all the previous time steps for back-propagation, which is space-consuming and limits the ability of batch processing.</li></ul><p>Then background section also talks about convolutional networks for sequencial modeling</p><ul><li>CNN operation is totally parallelizable and hence faster than RNN-like models, and it's also space-efficient.</li><li>However, CNN is not good at capturing the dependencies between the output and the input, especially when the distance between the two is large. It has something to do with the concept of receptive field. To capture long-range dependencies, we need to stack multiple layers of CNN, which is not efficient.</li></ul><p>To address the drawbacks mentioned above, the authors propose Transformer, which is based solely on attention mechanisms</p><ul><li>No recurrence is included thus parallelizable</li><li>attention machenism can draw global dependencies between the input and the output no matter how far they are from each other</li></ul><h3 id="scaled-dot-attention">Scaled dot attention</h3><p>The core principle of attention is how to map a query and a set of key-value pairs to an output. The output is a weighted sum of the values. The formula is given by <span class="math display">\[\operatorname{Attention}(Q,K,V)=\operatorname{softmax}({\frac{Q K^{T} }{\sqrt{d_{k} } } })V\]</span> Note that dot-product attention is much faster and more space-efficient in ractice</p><hr /><ul><li>query <span class="math inline">\(\bf q\)</span>: The row of matrix <span class="math inline">\(Q\)</span>, and thus <span class="math inline">\(Q\)</span> has the size of <span class="math inline">\(n\times d_k\)</span>, in which <span class="math inline">\(n\)</span> is the number of queries. The query vector has a dimension of <span class="math inline">\(d_k\)</span> for each corresponding embedded token. <span class="math inline">\(d_k\)</span> is actually much smaller than the dimension of an embedded vector. The vector <span class="math inline">\(\bf q\)</span> is linear projected from the corresponding embedded vector <span class="math inline">\(\bf e\)</span> <span class="math display">\[\bf q=W_q e\]</span> in which <span class="math inline">\(\bf W_q\)</span> is a learnable matrix.</li><li>key <span class="math inline">\(\bf k\)</span>: The key vector also has a dimension of <span class="math inline">\(d_k\)</span> for each unit and can be calculated by <span class="math display">\[\bf k=W_k e\]</span> in which <span class="math inline">\(\bf W_k\)</span> is also learnable</li><li>The dot product of <span class="math inline">\(\bf q\)</span> and <span class="math inline">\(\bf k\)</span>: Intuitively, <span class="math inline">\(\bf q\)</span> serves as an active query on behalf of each unit (token, patch...) and <span class="math inline">\(\bf k\)</span> is a key waiting for query from <span class="math inline">\(\bf q\)</span>. The scalar value <span class="math inline">\(\bf \langle q, k\rangle\)</span> measures the extent to which the unit represented by <span class="math inline">\(\bf q\)</span> matches that represented by <span class="math inline">\(\bf k\)</span>. By backpropagation and optimization, matrices <span class="math inline">\(\bf W_q\)</span> and <span class="math inline">\(\bf W_k\)</span> can capture a proper dependencies between each unit. <span class="math inline">\(\bf \langle q, k\rangle\)</span> is also called attention score</li><li><p>Division of <span class="math inline">\(\sqrt d_k\)</span>: The author says that for large values of <span class="math inline">\(d_k\)</span>, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients</p></li><li>Softmax: The softmax operation is conducted on <span class="math inline">\(\frac{QK^T}{\sqrt{d_k} }\)</span>, which is an <span class="math inline">\(n\times n\)</span> matrix, along the dimension of <span class="math inline">\(\bf k\)</span>, i.e., Given one <span class="math inline">\(\bf q_i\)</span>Ôºå <span class="math inline">\(\sum\nolimits_{j = 1}^{ {d_k} } {\left\langle { { {\bf{q} }_i},{ {\bf{k} }_j} } \right\rangle } = 1\)</span></li><li>Mask operation: <span class="math inline">\(\bf q_i\)</span> should only query keys that locate before it, i.e. <span class="math display">\[\left\langle { { {\bf{q} }_i},{ {\bf{k} }_j} } \right\rangle  = \left\{ \begin{array}{l}\left\langle { { {\bf{q} }_i},{ {\bf{k} }_j} } \right\rangle ,i \le j\\0,i &gt; j\end{array} \right.\]</span> The mask operation should be conducted ahead of softmax</li><li><p>value <span class="math inline">\(\bf v\)</span>: The value vector has a dimension of <span class="math inline">\(d_v\)</span>, which equals to the dimension of embedded vectors, for each unit, <span class="math inline">\(\bf v\)</span> is calculated by <span class="math display">\[\bf v=W_ve\]</span> Given that <span class="math inline">\(d_k &lt;&lt; d_v\)</span>, thus for the three matrices <span class="math inline">\({\left( { {W_v} } \right)_{ {d_v} \times {d_v} } },{\left( { {W_k} } \right)_{ {d_k} \times {d_v} }},{\left( { {W_q} } \right)_{ {d_k} \times {d_v} } }\)</span> <span class="math display">\[\operatorname{param}(W_v)&gt;&gt;\operatorname{param}(W_q) + \operatorname{param}(W_k)\]</span> Thus it's better to decompose <span class="math inline">\(W_v\)</span> into the multiplication of two low-rank matrices. Each value <span class="math inline">\(\bf v_i\)</span> is associated with corresponding key <span class="math inline">\(\bf k_i\)</span>. The output of attention for each unit is actually the weighted sum of all value vectors, serving as the variation of embedded vectors <span class="math display">\[{\mathop{\rm Attention}\nolimits} (Q,K,V) = {\mathop{\rm softmax}\nolimits} (\frac{ {Q{K^T} } }{ {\sqrt { {d_k} } } })V = {\left[ { {\mathop{\rm softmax}\nolimits} (\frac{ {Q{K^T} } }{ \sqrt {d_k}  })} \right]_{n \times n} }{V_{n \times {d_v} } } = {\left[ {\begin{array}{c}{\Delta {E_1} }\\{\Delta {E_2} }\\\vdots \\{\Delta {E_n} }\end{array} } \right]_{n \times {d_v} } }\]</span> Calculating <span class="math inline">\(E_i+\Delta E_i\)</span> gives the prediction of word embedding</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Paper Reading </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Paper Reading </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2024/10/06/hello-world/"/>
      <url>/2024/10/06/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Hi~</p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
